{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing\n",
    "#conda create -n algeria_economic_monitoring python=3.11\n",
    "#conda activate algeria_economic_monitoring\n",
    "#conda install -c conda-forge geopandas shapely pyproj mercantile tqdm fastparquet pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALGERIA OOKLA DATA PREPROCESSING PIPELINE\n",
      "  Loaded national boundary (ADM0)\n",
      "  Loaded ADM1: 48 wilayas\n",
      "  Loaded ADM2: 48 districts\n",
      "  Loaded ADM3: 1540 communes\n",
      "\n",
      "[STEP 2] Generating quadkey filter (z10 → expand to z16) and z12 grid...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  • Processing zoom 10 tiles: 100%|██████████| 3600/3600 [00:00<00:00, 21936.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  • Expanding to zoom 16 quadkeys (Ookla native resolution)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  • Expanding quadkeys: 100%|██████████| 1963/1963 [00:02<00:00, 695.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Generated 8,040,448 z16 quadkeys for Algeria\n",
      "  ✓ Saved: ./processed_data/algeria_quadkeys_z16.csv\n",
      "\n",
      "[STEP 2B] Creating z12 grid (polygons) for mapping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  • Processing zoom 12 tiles: 100%|██████████| 55696/55696 [00:00<00:00, 66016.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Created 31390 z12 tiles\n",
      "  ✓ Saved grid: ./processed_data/algeria_grid_z12_wkt.csv\n",
      "\n",
      "[STEP 2C] Building tile→admin (ADM1, ADM2 & ADM3) lookup...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mw/h5ql29r94gjgcwsnrl9_crlr0000gn/T/ipykernel_62151/2624865172.py:307: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  gdf_grid_z12_centroids[\"geometry\"] = gdf_grid_z12_centroids.geometry.centroid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Saved: ./processed_data/tile_admin_lookup_z12.csv\n",
      "  ✓ Found 26 parquet file(s) within 2019-2025\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "Processing 2019-Q1 (1 file(s))\n",
      "  • Loaded: 3,231,245 rows; columns: ['quadkey', 'tile', 'avg_d_kbps', 'avg_u_kbps', 'avg_lat_ms', 'tests', 'devices']\n",
      "  ⚠ Warning: 2019-Q1 has 1105 extreme latency values (>1000ms)\n",
      "  • 'network' column not found; proceeding (source assumed mobile).\n",
      "  • Quadkey filter kept: 12,204\n",
      "  ✓ National mean: 6.27↓ / 4.81↑ Mbps\n",
      "  ✓ ADM1 aggregated: 48 rows\n",
      "  ✓ ADM2 aggregated: 47 rows\n",
      "  ✓ ADM3 aggregated: 1135 rows\n",
      "  ✓ Grid z12 long rows added: 1529\n",
      "  ✓ Grid z16 long rows added: 12204\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "Processing 2019-Q2 (1 file(s))\n",
      "  • Loaded: 3,340,189 rows; columns: ['quadkey', 'tile', 'avg_d_kbps', 'avg_u_kbps', 'avg_lat_ms', 'tests', 'devices']\n",
      "  ⚠ Warning: 2019-Q2 has 4 extreme download and 0 extreme upload values (>1 Gbps)\n",
      "  ⚠ Warning: 2019-Q2 has 1147 extreme latency values (>1000ms)\n",
      "  • 'network' column not found; proceeding (source assumed mobile).\n",
      "  • Quadkey filter kept: 11,255\n",
      "  ✓ National mean: 6.96↓ / 5.51↑ Mbps\n",
      "  ✓ ADM1 aggregated: 48 rows\n",
      "  ✓ ADM2 aggregated: 47 rows\n",
      "  ✓ ADM3 aggregated: 1142 rows\n",
      "  ✓ Grid z12 long rows added: 1496\n",
      "  ✓ Grid z16 long rows added: 11255\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "Processing 2019-Q3 (1 file(s))\n",
      "  • Loaded: 4,012,185 rows; columns: ['quadkey', 'tile', 'avg_d_kbps', 'avg_u_kbps', 'avg_lat_ms', 'tests', 'devices']\n",
      "  ⚠ Warning: 2019-Q3 has 49 extreme download and 0 extreme upload values (>1 Gbps)\n",
      "  ⚠ Warning: 2019-Q3 has 1159 extreme latency values (>1000ms)\n",
      "  • 'network' column not found; proceeding (source assumed mobile).\n",
      "  • Quadkey filter kept: 11,911\n",
      "  ✓ National mean: 7.40↓ / 5.85↑ Mbps\n",
      "  ✓ ADM1 aggregated: 48 rows\n",
      "  ✓ ADM2 aggregated: 47 rows\n",
      "  ✓ ADM3 aggregated: 1135 rows\n",
      "  ✓ Grid z12 long rows added: 1513\n",
      "  ✓ Grid z16 long rows added: 11911\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "Processing 2019-Q4 (1 file(s))\n",
      "  • Loaded: 3,799,244 rows; columns: ['quadkey', 'tile', 'avg_d_kbps', 'avg_u_kbps', 'avg_lat_ms', 'tests', 'devices']\n",
      "  ⚠ Warning: 2019-Q4 has 143 extreme download and 0 extreme upload values (>1 Gbps)\n",
      "  ⚠ Warning: 2019-Q4 has 897 extreme latency values (>1000ms)\n",
      "  • 'network' column not found; proceeding (source assumed mobile).\n",
      "  • Quadkey filter kept: 12,500\n",
      "  ✓ National mean: 7.56↓ / 6.29↑ Mbps\n",
      "  ✓ ADM1 aggregated: 48 rows\n",
      "  ✓ ADM2 aggregated: 47 rows\n",
      "  ✓ ADM3 aggregated: 1192 rows\n",
      "  ✓ Grid z12 long rows added: 1605\n",
      "  ✓ Grid z16 long rows added: 12500\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "Processing 2020-Q1 (1 file(s))\n",
      "  • Loaded: 3,911,903 rows; columns: ['quadkey', 'tile', 'tile_x', 'tile_y', 'avg_d_kbps', 'avg_u_kbps', 'avg_lat_ms', 'avg_lat_down_ms', 'avg_lat_up_ms', 'tests', 'devices']\n",
      "  ⚠ Warning: 2020-Q1 has 262 extreme download and 0 extreme upload values (>1 Gbps)\n",
      "  ⚠ Warning: 2020-Q1 has 1064 extreme latency values (>1000ms)\n",
      "  • 'network' column not found; proceeding (source assumed mobile).\n",
      "  • Quadkey filter kept: 13,590\n",
      "  ✓ National mean: 7.76↓ / 6.16↑ Mbps\n",
      "  ✓ ADM1 aggregated: 48 rows\n",
      "  ✓ ADM2 aggregated: 47 rows\n",
      "  ✓ ADM3 aggregated: 1233 rows\n",
      "  ✓ Grid z12 long rows added: 1692\n",
      "  ✓ Grid z16 long rows added: 13590\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "Processing 2020-Q2 (1 file(s))\n",
      "  • Loaded: 4,095,011 rows; columns: ['quadkey', 'tile', 'tile_x', 'tile_y', 'avg_d_kbps', 'avg_u_kbps', 'avg_lat_ms', 'avg_lat_down_ms', 'avg_lat_up_ms', 'tests', 'devices']\n",
      "  ⚠ Warning: 2020-Q2 has 885 extreme download and 0 extreme upload values (>1 Gbps)\n",
      "  ⚠ Warning: 2020-Q2 has 1340 extreme latency values (>1000ms)\n",
      "  • 'network' column not found; proceeding (source assumed mobile).\n",
      "  • Quadkey filter kept: 11,818\n",
      "  ✓ National mean: 8.81↓ / 6.12↑ Mbps\n",
      "  ✓ ADM1 aggregated: 48 rows\n",
      "  ✓ ADM2 aggregated: 47 rows\n",
      "  ✓ ADM3 aggregated: 1151 rows\n",
      "  ✓ Grid z12 long rows added: 1509\n",
      "  ✓ Grid z16 long rows added: 11818\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "Processing 2020-Q3 (1 file(s))\n",
      "  • Loaded: 4,360,390 rows; columns: ['quadkey', 'tile', 'tile_x', 'tile_y', 'avg_d_kbps', 'avg_u_kbps', 'avg_lat_ms', 'avg_lat_down_ms', 'avg_lat_up_ms', 'tests', 'devices']\n",
      "  ⚠ Warning: 2020-Q3 has 2125 extreme download and 0 extreme upload values (>1 Gbps)\n",
      "  ⚠ Warning: 2020-Q3 has 1210 extreme latency values (>1000ms)\n",
      "  • 'network' column not found; proceeding (source assumed mobile).\n",
      "  • Quadkey filter kept: 13,643\n",
      "  ✓ National mean: 10.67↓ / 7.61↑ Mbps\n",
      "  ✓ ADM1 aggregated: 48 rows\n",
      "  ✓ ADM2 aggregated: 47 rows\n",
      "  ✓ ADM3 aggregated: 1222 rows\n",
      "  ✓ Grid z12 long rows added: 1669\n",
      "  ✓ Grid z16 long rows added: 13643\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "Processing 2020-Q4 (1 file(s))\n",
      "  • Loaded: 4,218,826 rows; columns: ['quadkey', 'tile', 'tile_x', 'tile_y', 'avg_d_kbps', 'avg_u_kbps', 'avg_lat_ms', 'avg_lat_down_ms', 'avg_lat_up_ms', 'tests', 'devices']\n",
      "  ⚠ Warning: 2020-Q4 has 2465 extreme download and 0 extreme upload values (>1 Gbps)\n",
      "  ⚠ Warning: 2020-Q4 has 815 extreme latency values (>1000ms)\n",
      "  • 'network' column not found; proceeding (source assumed mobile).\n",
      "  • Quadkey filter kept: 14,122\n",
      "  ✓ National mean: 11.82↓ / 8.29↑ Mbps\n",
      "  ✓ ADM1 aggregated: 48 rows\n",
      "  ✓ ADM2 aggregated: 47 rows\n",
      "  ✓ ADM3 aggregated: 1225 rows\n",
      "  ✓ Grid z12 long rows added: 1714\n",
      "  ✓ Grid z16 long rows added: 14122\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "Processing 2021-Q1 (1 file(s))\n",
      "  • Loaded: 4,193,475 rows; columns: ['quadkey', 'tile', 'tile_x', 'tile_y', 'avg_d_kbps', 'avg_u_kbps', 'avg_lat_ms', 'avg_lat_down_ms', 'avg_lat_up_ms', 'tests', 'devices']\n",
      "  ⚠ Warning: 2021-Q1 has 2509 extreme download and 0 extreme upload values (>1 Gbps)\n",
      "  ⚠ Warning: 2021-Q1 has 806 extreme latency values (>1000ms)\n",
      "  • 'network' column not found; proceeding (source assumed mobile).\n",
      "  • Quadkey filter kept: 16,479\n",
      "  ✓ National mean: 13.34↓ / 8.76↑ Mbps\n",
      "  ✓ ADM1 aggregated: 48 rows\n",
      "  ✓ ADM2 aggregated: 47 rows\n",
      "  ✓ ADM3 aggregated: 1270 rows\n",
      "  ✓ Grid z12 long rows added: 1933\n",
      "  ✓ Grid z16 long rows added: 16479\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "Processing 2021-Q2 (1 file(s))\n",
      "  • Loaded: 4,277,111 rows; columns: ['quadkey', 'tile', 'tile_x', 'tile_y', 'avg_d_kbps', 'avg_u_kbps', 'avg_lat_ms', 'avg_lat_down_ms', 'avg_lat_up_ms', 'tests', 'devices']\n",
      "  ⚠ Warning: 2021-Q2 has 2745 extreme download and 0 extreme upload values (>1 Gbps)\n",
      "  ⚠ Warning: 2021-Q2 has 906 extreme latency values (>1000ms)\n",
      "  • 'network' column not found; proceeding (source assumed mobile).\n",
      "  • Quadkey filter kept: 14,721\n",
      "  ✓ National mean: 16.35↓ / 9.46↑ Mbps\n",
      "  ✓ ADM1 aggregated: 48 rows\n",
      "  ✓ ADM2 aggregated: 47 rows\n",
      "  ✓ ADM3 aggregated: 1258 rows\n",
      "  ✓ Grid z12 long rows added: 1831\n",
      "  ✓ Grid z16 long rows added: 14721\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "Processing 2021-Q3 (1 file(s))\n",
      "  • Loaded: 4,383,948 rows; columns: ['quadkey', 'tile', 'avg_d_kbps', 'avg_u_kbps', 'avg_lat_ms', 'tests', 'devices']\n",
      "  ⚠ Warning: 2021-Q3 has 3044 extreme download and 0 extreme upload values (>1 Gbps)\n",
      "  ⚠ Warning: 2021-Q3 has 988 extreme latency values (>1000ms)\n",
      "  • 'network' column not found; proceeding (source assumed mobile).\n",
      "  • Quadkey filter kept: 16,516\n",
      "  ✓ National mean: 16.07↓ / 9.64↑ Mbps\n",
      "  ✓ ADM1 aggregated: 48 rows\n",
      "  ✓ ADM2 aggregated: 47 rows\n",
      "  ✓ ADM3 aggregated: 1325 rows\n",
      "  ✓ Grid z12 long rows added: 1992\n",
      "  ✓ Grid z16 long rows added: 16516\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "Processing 2021-Q4 (1 file(s))\n",
      "  • Loaded: 4,148,786 rows; columns: ['quadkey', 'tile', 'tile_x', 'tile_y', 'avg_d_kbps', 'avg_u_kbps', 'avg_lat_ms', 'avg_lat_down_ms', 'avg_lat_up_ms', 'tests', 'devices']\n",
      "  ⚠ Warning: 2021-Q4 has 2833 extreme download and 0 extreme upload values (>1 Gbps)\n",
      "  ⚠ Warning: 2021-Q4 has 981 extreme latency values (>1000ms)\n",
      "  • 'network' column not found; proceeding (source assumed mobile).\n",
      "  • Quadkey filter kept: 17,114\n",
      "  ✓ National mean: 16.91↓ / 10.04↑ Mbps\n",
      "  ✓ ADM1 aggregated: 48 rows\n",
      "  ✓ ADM2 aggregated: 47 rows\n",
      "  ✓ ADM3 aggregated: 1333 rows\n",
      "  ✓ Grid z12 long rows added: 2061\n",
      "  ✓ Grid z16 long rows added: 17114\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "Processing 2022-Q1 (1 file(s))\n",
      "  • Loaded: 3,820,724 rows; columns: ['quadkey', 'tile', 'tile_x', 'tile_y', 'avg_d_kbps', 'avg_u_kbps', 'avg_lat_ms', 'avg_lat_down_ms', 'avg_lat_up_ms', 'tests', 'devices']\n",
      "  ⚠ Warning: 2022-Q1 has 2657 extreme download and 0 extreme upload values (>1 Gbps)\n",
      "  ⚠ Warning: 2022-Q1 has 1137 extreme latency values (>1000ms)\n",
      "  • 'network' column not found; proceeding (source assumed mobile).\n",
      "  • Quadkey filter kept: 16,805\n",
      "  ✓ National mean: 17.82↓ / 10.65↑ Mbps\n",
      "  ✓ ADM1 aggregated: 48 rows\n",
      "  ✓ ADM2 aggregated: 47 rows\n",
      "  ✓ ADM3 aggregated: 1304 rows\n",
      "  ✓ Grid z12 long rows added: 2033\n",
      "  ✓ Grid z16 long rows added: 16805\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "Processing 2022-Q2 (1 file(s))\n",
      "  • Loaded: 4,027,744 rows; columns: ['quadkey', 'tile', 'tile_x', 'tile_y', 'avg_d_kbps', 'avg_u_kbps', 'avg_lat_ms', 'avg_lat_down_ms', 'avg_lat_up_ms', 'tests', 'devices']\n",
      "  ⚠ Warning: 2022-Q2 has 2757 extreme download and 0 extreme upload values (>1 Gbps)\n",
      "  ⚠ Warning: 2022-Q2 has 1089 extreme latency values (>1000ms)\n",
      "  • 'network' column not found; proceeding (source assumed mobile).\n",
      "  • Quadkey filter kept: 17,374\n",
      "  ✓ National mean: 19.45↓ / 11.48↑ Mbps\n",
      "  ✓ ADM1 aggregated: 48 rows\n",
      "  ✓ ADM2 aggregated: 47 rows\n",
      "  ✓ ADM3 aggregated: 1311 rows\n",
      "  ✓ Grid z12 long rows added: 2061\n",
      "  ✓ Grid z16 long rows added: 17374\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "Processing 2022-Q3 (1 file(s))\n",
      "  • Loaded: 4,046,154 rows; columns: ['quadkey', 'tile', 'tile_x', 'tile_y', 'avg_d_kbps', 'avg_u_kbps', 'avg_lat_ms', 'avg_lat_down_ms', 'avg_lat_up_ms', 'tests', 'devices']\n",
      "  ⚠ Warning: 2022-Q3 has 3024 extreme download and 0 extreme upload values (>1 Gbps)\n",
      "  ⚠ Warning: 2022-Q3 has 991 extreme latency values (>1000ms)\n",
      "  • 'network' column not found; proceeding (source assumed mobile).\n",
      "  • Quadkey filter kept: 17,442\n",
      "  ✓ National mean: 19.67↓ / 11.70↑ Mbps\n",
      "  ✓ ADM1 aggregated: 48 rows\n",
      "  ✓ ADM2 aggregated: 47 rows\n",
      "  ✓ ADM3 aggregated: 1320 rows\n",
      "  ✓ Grid z12 long rows added: 2065\n",
      "  ✓ Grid z16 long rows added: 17442\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "Processing 2022-Q4 (1 file(s))\n",
      "  • Loaded: 3,838,065 rows; columns: ['quadkey', 'tile', 'tile_x', 'tile_y', 'avg_d_kbps', 'avg_u_kbps', 'avg_lat_ms', 'avg_lat_down_ms', 'avg_lat_up_ms', 'tests', 'devices']\n",
      "  ⚠ Warning: 2022-Q4 has 2970 extreme download and 0 extreme upload values (>1 Gbps)\n",
      "  ⚠ Warning: 2022-Q4 has 1264 extreme latency values (>1000ms)\n",
      "  • 'network' column not found; proceeding (source assumed mobile).\n",
      "  • Quadkey filter kept: 18,783\n",
      "  ✓ National mean: 22.96↓ / 12.76↑ Mbps\n",
      "  ✓ ADM1 aggregated: 48 rows\n",
      "  ✓ ADM2 aggregated: 47 rows\n",
      "  ✓ ADM3 aggregated: 1346 rows\n",
      "  ✓ Grid z12 long rows added: 2208\n",
      "  ✓ Grid z16 long rows added: 18783\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "Processing 2023-Q1 (1 file(s))\n",
      "  • Loaded: 3,728,229 rows; columns: ['quadkey', 'tile', 'tile_x', 'tile_y', 'avg_d_kbps', 'avg_u_kbps', 'avg_lat_ms', 'avg_lat_down_ms', 'avg_lat_up_ms', 'tests', 'devices']\n",
      "  ⚠ Warning: 2023-Q1 has 2828 extreme download and 1 extreme upload values (>1 Gbps)\n",
      "  ⚠ Warning: 2023-Q1 has 1174 extreme latency values (>1000ms)\n",
      "  • 'network' column not found; proceeding (source assumed mobile).\n",
      "  • Quadkey filter kept: 17,670\n",
      "  ✓ National mean: 24.11↓ / 12.64↑ Mbps\n",
      "  ✓ ADM1 aggregated: 48 rows\n",
      "  ✓ ADM2 aggregated: 47 rows\n",
      "  ✓ ADM3 aggregated: 1355 rows\n",
      "  ✓ Grid z12 long rows added: 2189\n",
      "  ✓ Grid z16 long rows added: 17670\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "Processing 2023-Q2 (1 file(s))\n",
      "  • Loaded: 3,864,546 rows; columns: ['quadkey', 'tile', 'tile_x', 'tile_y', 'avg_d_kbps', 'avg_u_kbps', 'avg_lat_ms', 'avg_lat_down_ms', 'avg_lat_up_ms', 'tests', 'devices']\n",
      "  ⚠ Warning: 2023-Q2 has 3498 extreme download and 1 extreme upload values (>1 Gbps)\n",
      "  ⚠ Warning: 2023-Q2 has 1099 extreme latency values (>1000ms)\n",
      "  • 'network' column not found; proceeding (source assumed mobile).\n",
      "  • Quadkey filter kept: 16,764\n",
      "  ✓ National mean: 27.36↓ / 13.37↑ Mbps\n",
      "  ✓ ADM1 aggregated: 48 rows\n",
      "  ✓ ADM2 aggregated: 47 rows\n",
      "  ✓ ADM3 aggregated: 1316 rows\n",
      "  ✓ Grid z12 long rows added: 2031\n",
      "  ✓ Grid z16 long rows added: 16764\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "Processing 2023-Q3 (1 file(s))\n",
      "  • Loaded: 4,005,796 rows; columns: ['quadkey', 'tile', 'tile_x', 'tile_y', 'avg_d_kbps', 'avg_u_kbps', 'avg_lat_ms', 'avg_lat_down_ms', 'avg_lat_up_ms', 'tests', 'devices']\n",
      "  ⚠ Warning: 2023-Q3 has 4307 extreme download and 2 extreme upload values (>1 Gbps)\n",
      "  ⚠ Warning: 2023-Q3 has 1112 extreme latency values (>1000ms)\n",
      "  • 'network' column not found; proceeding (source assumed mobile).\n",
      "  • Quadkey filter kept: 16,790\n",
      "  ✓ National mean: 28.77↓ / 13.90↑ Mbps\n",
      "  ✓ ADM1 aggregated: 48 rows\n",
      "  ✓ ADM2 aggregated: 47 rows\n",
      "  ✓ ADM3 aggregated: 1311 rows\n",
      "  ✓ Grid z12 long rows added: 1976\n",
      "  ✓ Grid z16 long rows added: 16790\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "Processing 2023-Q4 (1 file(s))\n",
      "  • Loaded: 3,771,204 rows; columns: ['quadkey', 'tile', 'tile_x', 'tile_y', 'avg_d_kbps', 'avg_u_kbps', 'avg_lat_ms', 'avg_lat_down_ms', 'avg_lat_up_ms', 'tests', 'devices']\n",
      "  ⚠ Warning: 2023-Q4 has 5841 extreme download and 0 extreme upload values (>1 Gbps)\n",
      "  ⚠ Warning: 2023-Q4 has 957 extreme latency values (>1000ms)\n",
      "  • 'network' column not found; proceeding (source assumed mobile).\n",
      "  • Quadkey filter kept: 17,443\n",
      "  ✓ National mean: 30.01↓ / 13.88↑ Mbps\n",
      "  ✓ ADM1 aggregated: 48 rows\n",
      "  ✓ ADM2 aggregated: 47 rows\n",
      "  ✓ ADM3 aggregated: 1312 rows\n",
      "  ✓ Grid z12 long rows added: 2021\n",
      "  ✓ Grid z16 long rows added: 17443\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "Processing 2024-Q1 (1 file(s))\n",
      "  • Loaded: 3,674,000 rows; columns: ['quadkey', 'tile', 'tile_x', 'tile_y', 'avg_d_kbps', 'avg_u_kbps', 'avg_lat_ms', 'avg_lat_down_ms', 'avg_lat_up_ms', 'tests', 'devices']\n",
      "  ⚠ Warning: 2024-Q1 has 7692 extreme download and 1 extreme upload values (>1 Gbps)\n",
      "  ⚠ Warning: 2024-Q1 has 884 extreme latency values (>1000ms)\n",
      "  • 'network' column not found; proceeding (source assumed mobile).\n",
      "  • Quadkey filter kept: 17,394\n",
      "  ✓ National mean: 31.70↓ / 13.92↑ Mbps\n",
      "  ✓ ADM1 aggregated: 48 rows\n",
      "  ✓ ADM2 aggregated: 47 rows\n",
      "  ✓ ADM3 aggregated: 1318 rows\n",
      "  ✓ Grid z12 long rows added: 2075\n",
      "  ✓ Grid z16 long rows added: 17394\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "Processing 2024-Q2 (1 file(s))\n",
      "  • Loaded: 3,703,161 rows; columns: ['quadkey', 'tile', 'tile_x', 'tile_y', 'avg_d_kbps', 'avg_u_kbps', 'avg_lat_ms', 'avg_lat_down_ms', 'avg_lat_up_ms', 'tests', 'devices']\n",
      "  ⚠ Warning: 2024-Q2 has 9768 extreme download and 3 extreme upload values (>1 Gbps)\n",
      "  ⚠ Warning: 2024-Q2 has 1161 extreme latency values (>1000ms)\n",
      "  • 'network' column not found; proceeding (source assumed mobile).\n",
      "  • Quadkey filter kept: 17,487\n",
      "  ✓ National mean: 34.03↓ / 13.66↑ Mbps\n",
      "  ✓ ADM1 aggregated: 48 rows\n",
      "  ✓ ADM2 aggregated: 47 rows\n",
      "  ✓ ADM3 aggregated: 1324 rows\n",
      "  ✓ Grid z12 long rows added: 2070\n",
      "  ✓ Grid z16 long rows added: 17487\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "Processing 2024-Q3 (1 file(s))\n",
      "  • Loaded: 3,773,658 rows; columns: ['quadkey', 'tile', 'tile_x', 'tile_y', 'avg_d_kbps', 'avg_u_kbps', 'avg_lat_ms', 'avg_lat_down_ms', 'avg_lat_up_ms', 'tests', 'devices']\n",
      "  ⚠ Warning: 2024-Q3 has 10541 extreme download and 0 extreme upload values (>1 Gbps)\n",
      "  ⚠ Warning: 2024-Q3 has 1131 extreme latency values (>1000ms)\n",
      "  • 'network' column not found; proceeding (source assumed mobile).\n",
      "  • Quadkey filter kept: 17,106\n",
      "  ✓ National mean: 33.26↓ / 13.31↑ Mbps\n",
      "  ✓ ADM1 aggregated: 48 rows\n",
      "  ✓ ADM2 aggregated: 47 rows\n",
      "  ✓ ADM3 aggregated: 1318 rows\n",
      "  ✓ Grid z12 long rows added: 2078\n",
      "  ✓ Grid z16 long rows added: 17106\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "Processing 2024-Q4 (1 file(s))\n",
      "  • Loaded: 3,551,267 rows; columns: ['quadkey', 'tile', 'tile_x', 'tile_y', 'avg_d_kbps', 'avg_u_kbps', 'avg_lat_ms', 'avg_lat_down_ms', 'avg_lat_up_ms', 'tests', 'devices']\n",
      "  ⚠ Warning: 2024-Q4 has 11787 extreme download and 2 extreme upload values (>1 Gbps)\n",
      "  ⚠ Warning: 2024-Q4 has 1097 extreme latency values (>1000ms)\n",
      "  • 'network' column not found; proceeding (source assumed mobile).\n",
      "  • Quadkey filter kept: 17,370\n",
      "  ✓ National mean: 33.16↓ / 12.86↑ Mbps\n",
      "  ✓ ADM1 aggregated: 48 rows\n",
      "  ✓ ADM2 aggregated: 47 rows\n",
      "  ✓ ADM3 aggregated: 1335 rows\n",
      "  ✓ Grid z12 long rows added: 2081\n",
      "  ✓ Grid z16 long rows added: 17370\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "Processing 2025-Q1 (1 file(s))\n",
      "  • Loaded: 3,388,115 rows; columns: ['quadkey', 'tile', 'tile_x', 'tile_y', 'avg_d_kbps', 'avg_u_kbps', 'avg_lat_ms', 'avg_lat_down_ms', 'avg_lat_up_ms', 'tests', 'devices']\n",
      "  ⚠ Warning: 2025-Q1 has 12904 extreme download and 3 extreme upload values (>1 Gbps)\n",
      "  ⚠ Warning: 2025-Q1 has 1121 extreme latency values (>1000ms)\n",
      "  • 'network' column not found; proceeding (source assumed mobile).\n",
      "  • Quadkey filter kept: 16,295\n",
      "  ✓ National mean: 35.62↓ / 13.16↑ Mbps\n",
      "  ✓ ADM1 aggregated: 48 rows\n",
      "  ✓ ADM2 aggregated: 47 rows\n",
      "  ✓ ADM3 aggregated: 1301 rows\n",
      "  ✓ Grid z12 long rows added: 2038\n",
      "  ✓ Grid z16 long rows added: 16295\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "Processing 2025-Q2 (1 file(s))\n",
      "  • Loaded: 3,514,872 rows; columns: ['quadkey', 'tile', 'tile_x', 'tile_y', 'avg_d_kbps', 'avg_u_kbps', 'avg_lat_ms', 'avg_lat_down_ms', 'avg_lat_up_ms', 'tests', 'devices']\n",
      "  ⚠ Warning: 2025-Q2 has 13896 extreme download and 6 extreme upload values (>1 Gbps)\n",
      "  ⚠ Warning: 2025-Q2 has 843 extreme latency values (>1000ms)\n",
      "  • 'network' column not found; proceeding (source assumed mobile).\n",
      "  • Quadkey filter kept: 17,698\n",
      "  ✓ National mean: 35.92↓ / 13.37↑ Mbps\n",
      "  ✓ ADM1 aggregated: 48 rows\n",
      "  ✓ ADM2 aggregated: 47 rows\n",
      "  ✓ ADM3 aggregated: 1372 rows\n",
      "  ✓ Grid z12 long rows added: 2204\n",
      "  ✓ Grid z16 long rows added: 17698\n",
      "  ✓ National trends: 26 periods → ./processed_data/algeria_national_trends_mobile.csv\n",
      "  ✓ Subnational trends: 35634 rows → ./processed_data/algeria_subnational_trends_mobile.csv\n",
      "  ✓ Grid long z12: 49674 rows → ./processed_data/algeria_grid_data_long_z12_mobile.csv\n",
      "\n",
      "[STEP 5B] Building z12 wide GeoParquet for mapping...\n",
      "  ✓ z12 time-series GeoParquet → ./processed_data/algeria_grid_timeseries_mobile_wkt.csv\n",
      "  ✓ Grid long z16: 408294 rows → ./processed_data/algeria_grid_data_long_z16_mobile.csv\n",
      "PREPROCESSING COMPLETE\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import re\n",
    "from itertools import product\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, box, shape\n",
    "import mercantile\n",
    "from tqdm import tqdm\n",
    "\n",
    "# CONFIG\n",
    "PATH_DATA = \"./ookle_algeria/\"  # Ookla parquet root\n",
    "OUTPUT_DIR = \"./processed_data/\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "NET_TYPE = \"mobile\"  # Filter for mobile data only when necessary\n",
    "ISO_CODE = \"DZA\"  # Country specification\n",
    "YEAR_MIN, YEAR_MAX = 2019, 2025\n",
    "\n",
    "# Boundaries\n",
    "PATH_ADM0 = \"./boundaries/geoBoundaries-DZA-ADM0.geojson\"\n",
    "PATH_ADM1 = \"./boundaries/geoBoundaries-DZA-ADM1.geojson\"\n",
    "PATH_ADM2 = \"./boundaries/geoBoundaries-DZA-ADM2.geojson\"\n",
    "PATH_ADM3 = \"./boundaries/geoBoundaries-DZA-ADM3.geojson\"\n",
    "\n",
    "print(\"ALGERIA OOKLA DATA PREPROCESSING PIPELINE\")\n",
    "\n",
    "# UTILITIES\n",
    "\n",
    "def tile_to_quadkey(x: int, y: int, z: int) -> str:\n",
    "    qk = \"\"\n",
    "    for i in range(z, 0, -1):\n",
    "        digit = 0\n",
    "        mask = 1 << (i - 1)\n",
    "        if (x & mask) != 0:\n",
    "            digit += 1\n",
    "        if (y & mask) != 0:\n",
    "            digit += 2\n",
    "        qk += str(digit)\n",
    "    return qk\n",
    "\n",
    "def quadkey_to_point(quadkey: str):\n",
    "    t = mercantile.quadkey_to_tile(quadkey)\n",
    "    b = mercantile.bounds(t)\n",
    "    return Point((b.west + b.east) / 2, (b.south + b.north) / 2)\n",
    "\n",
    "def quadkey_to_polygon(quadkey: str):\n",
    "    t = mercantile.quadkey_to_tile(quadkey)\n",
    "    b = mercantile.bounds(t)\n",
    "    return box(b.west, b.south, b.east, b.north)\n",
    "\n",
    "def get_country_quadkeys_at_zoom(boundary_gdf: gpd.GeoDataFrame, zoom_level: int) -> list:\n",
    "    \"\"\"Return quadkeys whose TILE CENTROIDS fall inside the country (centroid-within).\"\"\"\n",
    "    minx, miny, maxx, maxy = boundary_gdf.total_bounds\n",
    "    tiles = list(mercantile.tiles(minx, miny, maxx, maxy, zoom_level))\n",
    "\n",
    "    quadkeys, pts = [], []\n",
    "    for t in tqdm(tiles, desc=f\"  • Processing zoom {zoom_level} tiles\"):\n",
    "        qk = tile_to_quadkey(t.x, t.y, t.z)\n",
    "        quadkeys.append(qk)\n",
    "        pts.append(quadkey_to_point(qk))   # centroid\n",
    "\n",
    "    gdf = gpd.GeoDataFrame({\"quadkey\": quadkeys, \"geometry\": pts}, crs=\"EPSG:4326\")\n",
    "    gdf_f = gpd.sjoin(gdf, boundary_gdf[[\"geometry\"]], how=\"inner\", predicate=\"within\")\n",
    "    return gdf_f[\"quadkey\"].astype(str).tolist()\n",
    "\n",
    "def get_subquadkeys(parent_quadkey: str, target_zoom: int) -> list:\n",
    "    current_zoom = len(parent_quadkey)\n",
    "    delta = target_zoom - current_zoom\n",
    "    if delta < 0:\n",
    "        raise ValueError(\"Target zoom must be >= parent zoom\")\n",
    "    if delta == 0:\n",
    "        return [parent_quadkey]\n",
    "    out = []\n",
    "    for suffix in product(\"0123\", repeat=delta):\n",
    "        out.append(parent_quadkey + \"\".join(suffix))\n",
    "    return out\n",
    "\n",
    "def ensure_tile_polygon_gdf(df: pd.DataFrame) -> gpd.GeoDataFrame:\n",
    "    if \"quadkey\" in df.columns:\n",
    "        df = df.copy()\n",
    "        df[\"geometry\"] = df[\"quadkey\"].astype(str).apply(quadkey_to_polygon)\n",
    "    elif {\"lat\", \"lon\"}.issubset(df.columns):\n",
    "        df = df.copy()\n",
    "        df[\"geometry\"] = [Point(xy) for xy in zip(df[\"lon\"], df[\"lat\"])]\n",
    "    else:\n",
    "        raise ValueError(\"No geometry source (quadkey or lat/lon) found in dataframe.\")\n",
    "    return gpd.GeoDataFrame(df, crs=\"EPSG:4326\")\n",
    "\n",
    "def _flatten_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    new_cols = []\n",
    "    for col in df.columns:\n",
    "        if isinstance(col, tuple):\n",
    "            a, b = col\n",
    "            new_cols.append(f\"{a}_{b}\" if b else a)\n",
    "        else:\n",
    "            new_cols.append(col)\n",
    "    df.columns = new_cols\n",
    "    return df\n",
    "\n",
    "def validate_ookla_data(df: pd.DataFrame, year: int, quarter: int) -> bool:\n",
    "    \"\"\"Validate Ookla parquet data before processing\"\"\"\n",
    "    required_cols = [\"avg_d_kbps\", \"avg_u_kbps\"]\n",
    "    missing = [c for c in required_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"{year}-Q{quarter}: Missing required columns: {missing}\")\n",
    "\n",
    "    if len(df) == 0:\n",
    "        print(f\"  ⚠ Warning: {year}-Q{quarter} has no rows\")\n",
    "        return False\n",
    "\n",
    "    neg_down = (df[\"avg_d_kbps\"] < 0).sum()\n",
    "    neg_up = (df[\"avg_u_kbps\"] < 0).sum()\n",
    "    if neg_down > 0 or neg_up > 0:\n",
    "        print(f\"  ⚠ Warning: {year}-Q{quarter} has {neg_down} negative download and {neg_up} negative upload values\")\n",
    "\n",
    "    extreme_down = (df[\"avg_d_kbps\"] > 1_000_000).sum()\n",
    "    extreme_up = (df[\"avg_u_kbps\"] > 1_000_000).sum()\n",
    "    if extreme_down > 0 or extreme_up > 0:\n",
    "        print(f\"  ⚠ Warning: {year}-Q{quarter} has {extreme_down} extreme download and {extreme_up} extreme upload values (>1 Gbps)\")\n",
    "\n",
    "    if \"avg_lat_ms\" in df.columns:\n",
    "        neg_lat = (df[\"avg_lat_ms\"] < 0).sum()\n",
    "        extreme_lat = (df[\"avg_lat_ms\"] > 1000).sum()\n",
    "        if neg_lat > 0:\n",
    "            print(f\"  ⚠ Warning: {year}-Q{quarter} has {neg_lat} negative latency values\")\n",
    "        if extreme_lat > 0:\n",
    "            print(f\"  ⚠ Warning: {year}-Q{quarter} has {extreme_lat} extreme latency values (>1000ms)\")\n",
    "\n",
    "    return True\n",
    "\n",
    "def validate_boundaries(boundary_gdf: gpd.GeoDataFrame, level: str, required_cols: list) -> bool:\n",
    "    \"\"\"Validate boundary GeoDataFrame\"\"\"\n",
    "    missing = [c for c in required_cols if c not in boundary_gdf.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"{level}: Missing required columns: {missing}\")\n",
    "\n",
    "    invalid = (~boundary_gdf.geometry.is_valid).sum()\n",
    "    if invalid > 0:\n",
    "        print(f\"  ⚠ Warning: {level} has {invalid} invalid geometries (will be fixed with buffer(0))\")\n",
    "\n",
    "    return True\n",
    "\n",
    "def read_geojson_no_gdal(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        gj = json.load(f)\n",
    "    recs = []\n",
    "    for feat in gj[\"features\"]:\n",
    "        props = feat.get(\"properties\", {})\n",
    "        geom = shape(feat[\"geometry\"])\n",
    "        recs.append({**props, \"geometry\": geom})\n",
    "    return gpd.GeoDataFrame(recs, crs=\"EPSG:4326\")\n",
    "\n",
    "def aggregate_by_admin(\n",
    "    gdf_meas: gpd.GeoDataFrame,\n",
    "    admin_gdf: gpd.GeoDataFrame,\n",
    "    id_col: str,\n",
    "    name_col: str,\n",
    "    admin_level_tag: str,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Spatially join measurements to admin polygons (intersects) and aggregate.\n",
    "    Keeps both the boundary 'code' (id_col) and UTF-8 name (name_col).\n",
    "    \"\"\"\n",
    "    joined = gpd.sjoin(\n",
    "        gdf_meas,\n",
    "        admin_gdf[[id_col, name_col, \"geometry\"]],\n",
    "        how=\"left\",\n",
    "        predicate=\"intersects\",\n",
    "    )\n",
    "\n",
    "    has_tests = \"tests\" in joined.columns\n",
    "    has_devices = \"devices\" in joined.columns\n",
    "    has_latency = \"avg_lat_ms\" in joined.columns\n",
    "\n",
    "    agg_dict = {\n",
    "        \"avg_d_kbps\": [\"mean\", \"median\", \"count\"],\n",
    "        \"avg_u_kbps\": [\"mean\", \"median\"],\n",
    "    }\n",
    "    if has_latency:\n",
    "        agg_dict[\"avg_lat_ms\"] = [\"mean\", \"median\"]\n",
    "    if has_tests:\n",
    "        agg_dict[\"tests\"] = \"sum\"\n",
    "    if has_devices:\n",
    "        agg_dict[\"devices\"] = \"sum\"\n",
    "\n",
    "    out = joined.groupby([id_col, name_col]).agg(agg_dict).reset_index()\n",
    "    out = _flatten_columns(out)\n",
    "\n",
    "    rename_map = {\n",
    "        \"avg_d_kbps_mean\": \"avg_download_kbps\",\n",
    "        \"avg_d_kbps_median\": \"median_download_kbps\",\n",
    "        \"avg_d_kbps_count\": \"num_tiles\",\n",
    "        \"avg_u_kbps_mean\": \"avg_upload_kbps\",\n",
    "        \"avg_u_kbps_median\": \"median_upload_kbps\",\n",
    "        \"tests_sum\": \"num_tests\",\n",
    "        \"devices_sum\": \"num_devices\",\n",
    "        \"avg_lat_ms_mean\": \"avg_latency_ms\",\n",
    "        \"avg_lat_ms_median\": \"median_latency_ms\",\n",
    "    }\n",
    "    out = out.rename(columns=rename_map)\n",
    "\n",
    "    # Final tidy\n",
    "    out[\"admin_level\"] = admin_level_tag\n",
    "    out[\"admin_code\"] = out[id_col].astype(str)\n",
    "    out[\"admin_name\"] = out[name_col].astype(str)\n",
    "\n",
    "    # Mbps conversions (download/upload only)\n",
    "    if \"avg_download_kbps\" in out.columns:\n",
    "        out[\"avg_download_mbps\"] = out[\"avg_download_kbps\"] / 1000.0\n",
    "    if \"avg_upload_kbps\" in out.columns:\n",
    "        out[\"avg_upload_mbps\"] = out[\"avg_upload_kbps\"] / 1000.0\n",
    "    if \"median_download_kbps\" in out.columns:\n",
    "        out[\"median_download_mbps\"] = out[\"median_download_kbps\"] / 1000.0\n",
    "    if \"median_upload_kbps\" in out.columns:\n",
    "        out[\"median_upload_mbps\"] = out[\"median_upload_kbps\"] / 1000.0\n",
    "\n",
    "    keep = [\n",
    "        \"admin_level\", \"admin_code\", \"admin_name\",\n",
    "        \"avg_download_mbps\", \"avg_upload_mbps\",\n",
    "        \"median_download_mbps\", \"median_upload_mbps\",\n",
    "        \"num_tiles\",\n",
    "    ]\n",
    "    if \"avg_latency_ms\" in out.columns:\n",
    "        keep += [\"avg_latency_ms\", \"median_latency_ms\"]\n",
    "    if \"num_tests\" in out.columns:\n",
    "        keep.append(\"num_tests\")\n",
    "    if \"num_devices\" in out.columns:\n",
    "        keep.append(\"num_devices\")\n",
    "\n",
    "    return out[keep]\n",
    "\n",
    "# STEP 1: LOAD BOUNDARIES\n",
    "\n",
    "# Validate paths exist\n",
    "if not os.path.exists(PATH_ADM0):\n",
    "    raise FileNotFoundError(f\"ADM0 boundary file not found: {PATH_ADM0}\")\n",
    "if not os.path.exists(PATH_ADM1):\n",
    "    raise FileNotFoundError(f\"ADM1 boundary file not found: {PATH_ADM1}\")\n",
    "if not os.path.exists(PATH_ADM2):\n",
    "    raise FileNotFoundError(f\"ADM2 boundary file not found: {PATH_ADM2}\")\n",
    "if not os.path.exists(PATH_ADM3):\n",
    "    raise FileNotFoundError(f\"ADM3 boundary file not found: {PATH_ADM3}\")\n",
    "\n",
    "boundary_national = read_geojson_no_gdal(PATH_ADM0).to_crs(\"EPSG:4326\")\n",
    "print(\"  Loaded national boundary (ADM0)\")\n",
    "\n",
    "boundary_adm1 = read_geojson_no_gdal(PATH_ADM1)[[\"shapeISO\", \"shapeName\", \"geometry\"]].to_crs(\"EPSG:4326\")\n",
    "boundary_adm1[\"shapeName\"] = boundary_adm1[\"shapeName\"].astype(str)\n",
    "validate_boundaries(boundary_adm1, \"ADM1\", [\"shapeISO\", \"shapeName\", \"geometry\"])\n",
    "print(f\"  Loaded ADM1: {len(boundary_adm1)} wilayas\")\n",
    "\n",
    "boundary_adm2 = read_geojson_no_gdal(PATH_ADM2)[[\"shapeISO\", \"shapeName\", \"geometry\"]].to_crs(\"EPSG:4326\")\n",
    "boundary_adm2[\"shapeName\"] = boundary_adm2[\"shapeName\"].astype(str)\n",
    "validate_boundaries(boundary_adm2, \"ADM2\", [\"shapeISO\", \"shapeName\", \"geometry\"])\n",
    "print(f\"  Loaded ADM2: {len(boundary_adm2)} districts\")\n",
    "\n",
    "boundary_adm3 = read_geojson_no_gdal(PATH_ADM3)[[\"shapeISO\", \"shapeName\", \"geometry\"]].to_crs(\"EPSG:4326\")\n",
    "boundary_adm3[\"shapeName\"] = boundary_adm3[\"shapeName\"].astype(str)\n",
    "validate_boundaries(boundary_adm3, \"ADM3\", [\"shapeISO\", \"shapeName\", \"geometry\"])\n",
    "print(f\"  Loaded ADM3: {len(boundary_adm3)} communes\")\n",
    "\n",
    "# Fix invalid geometries\n",
    "for gdf_fix in [boundary_national, boundary_adm1, boundary_adm2, boundary_adm3]:\n",
    "    if isinstance(gdf_fix, gpd.GeoDataFrame):\n",
    "        gdf_fix.geometry = gdf_fix.buffer(0)\n",
    "\n",
    "ALGERIA_BOUNDS = boundary_national.total_bounds\n",
    "\n",
    "# STEP 2: QUADKEY FILTERS & Z12 GRID\n",
    "print(\"\\n[STEP 2] Generating quadkey filter (z10 → expand to z16) and z12 grid...\")\n",
    "\n",
    "parent_qk_z10 = get_country_quadkeys_at_zoom(boundary_national, zoom_level=10)\n",
    "\n",
    "print(\"  • Expanding to zoom 16 quadkeys (Ookla native resolution)...\")\n",
    "all_qk_z16 = []\n",
    "for pqk in tqdm(parent_qk_z10, desc=\"  • Expanding quadkeys\"):\n",
    "    all_qk_z16.extend(get_subquadkeys(pqk, 16))\n",
    "print(f\"  ✓ Generated {len(all_qk_z16):,} z16 quadkeys for Algeria\")\n",
    "\n",
    "pd.DataFrame({\"quadkey\": all_qk_z16, \"country\": ISO_CODE}).to_csv(\n",
    "    os.path.join(OUTPUT_DIR, \"algeria_quadkeys_z16.csv\"), index=False, encoding=\"utf-8\"\n",
    ")\n",
    "print(f\"  ✓ Saved: {OUTPUT_DIR}algeria_quadkeys_z16.csv\")\n",
    "\n",
    "QK16_SET = set(all_qk_z16)\n",
    "\n",
    "print(\"\\n[STEP 2B] Creating z12 grid (polygons) for mapping...\")\n",
    "parent_qk_z12 = get_country_quadkeys_at_zoom(boundary_national, zoom_level=12)\n",
    "gdf_grid_z12 = gpd.GeoDataFrame(\n",
    "    {\"quadkey\": parent_qk_z12, \"geometry\": [quadkey_to_polygon(qk) for qk in parent_qk_z12]},\n",
    "    crs=\"EPSG:4326\",\n",
    ")\n",
    "gdf_grid_z12[\"quadkey\"] = gdf_grid_z12[\"quadkey\"].astype(str)\n",
    "print(f\"  ✓ Created {len(gdf_grid_z12)} z12 tiles\")\n",
    "# Save as CSV with WKT geometry (avoids pyarrow requirement)\n",
    "_gz12 = gdf_grid_z12.copy()\n",
    "_gz12[\"wkt\"] = _gz12.geometry.apply(lambda g: g.wkt)\n",
    "_gz12.drop(columns=\"geometry\").to_csv(os.path.join(OUTPUT_DIR, \"algeria_grid_z12_wkt.csv\"), index=False, encoding=\"utf-8\")\n",
    "print(f\"  ✓ Saved grid: {OUTPUT_DIR}algeria_grid_z12_wkt.csv\")\n",
    "\n",
    "print(\"\\n[STEP 2C] Building tile→admin (ADM1, ADM2 & ADM3) lookup...\")\n",
    "# Use CENTROID-WITHIN for admin assignment to avoid border overlaps\n",
    "gdf_grid_z12_centroids = gdf_grid_z12.copy()\n",
    "gdf_grid_z12_centroids[\"geometry\"] = gdf_grid_z12_centroids.geometry.centroid\n",
    "\n",
    "adm1_ren = boundary_adm1.rename(columns={\"shapeISO\": \"adm1_code\", \"shapeName\": \"adm1_name\"})\n",
    "adm2_ren = boundary_adm2.rename(columns={\"shapeISO\": \"adm2_code\", \"shapeName\": \"adm2_name\"})\n",
    "adm3_ren = boundary_adm3.rename(columns={\"shapeISO\": \"adm3_code\", \"shapeName\": \"adm3_name\"})\n",
    "\n",
    "lk = gpd.sjoin(\n",
    "    gdf_grid_z12_centroids[[\"quadkey\", \"geometry\"]],\n",
    "    adm1_ren[[\"adm1_code\", \"adm1_name\", \"geometry\"]],\n",
    "    how=\"left\",\n",
    "    predicate=\"within\"\n",
    ").drop(columns=\"index_right\")\n",
    "lk = gpd.sjoin(\n",
    "    lk,\n",
    "    adm2_ren[[\"adm2_code\", \"adm2_name\", \"geometry\"]],\n",
    "    how=\"left\",\n",
    "    predicate=\"within\"\n",
    ").drop(columns=\"index_right\")\n",
    "lk = gpd.sjoin(\n",
    "    lk,\n",
    "    adm3_ren[[\"adm3_code\", \"adm3_name\", \"geometry\"]],\n",
    "    how=\"left\",\n",
    "    predicate=\"within\"\n",
    ").drop(columns=[\"index_right\", \"geometry\"])\n",
    "\n",
    "lk[\"quadkey_z12\"] = lk[\"quadkey\"].astype(str)\n",
    "lookup = lk[[\"quadkey_z12\", \"adm1_code\", \"adm1_name\", \"adm2_code\", \"adm2_name\", \"adm3_code\", \"adm3_name\"]]\n",
    "lookup.to_csv(os.path.join(OUTPUT_DIR, \"tile_admin_lookup_z12.csv\"), index=False, encoding=\"utf-8\")\n",
    "print(f\"  ✓ Saved: {OUTPUT_DIR}tile_admin_lookup_z12.csv\")\n",
    "\n",
    "# STEP 3: DISCOVER PARQUET FILES (2019–2025)\n",
    "\n",
    "available = []\n",
    "if not os.path.isdir(PATH_DATA):\n",
    "    raise FileNotFoundError(f\"Data directory not found: {PATH_DATA}\")\n",
    "\n",
    "# iterate only through directories and parse year with regex\n",
    "for year_folder in sorted(d for d in os.listdir(PATH_DATA) if os.path.isdir(os.path.join(PATH_DATA, d))):\n",
    "    if not year_folder.startswith(\"year=\"):\n",
    "        continue\n",
    "    m_year = re.search(r'year\\s*=\\s*(\\d+)', year_folder)\n",
    "    if not m_year:\n",
    "        continue\n",
    "    year = int(m_year.group(1))\n",
    "    if year < YEAR_MIN or year > YEAR_MAX:\n",
    "        continue\n",
    "\n",
    "    year_path = os.path.join(PATH_DATA, year_folder)\n",
    "    # iterate only through directories and parse quarter with regex\n",
    "    for quarter_folder in sorted(d for d in os.listdir(year_path) if os.path.isdir(os.path.join(year_path, d))):\n",
    "        if not quarter_folder.startswith(\"quarter=\"):\n",
    "            continue\n",
    "        m_quarter = re.search(r'quarter\\s*=\\s*(\\d+)', quarter_folder)\n",
    "        if not m_quarter:\n",
    "            continue\n",
    "        quarter = int(m_quarter.group(1))\n",
    "        quarter_path = os.path.join(year_path, quarter_folder)\n",
    "        files = sorted(glob.glob(os.path.join(quarter_path, \"*.parquet\")))\n",
    "        if files:\n",
    "            for fp in files:  # include all shards for accuracy\n",
    "                available.append((year, quarter, fp))\n",
    "\n",
    "print(f\"  ✓ Found {len(available)} parquet file(s) within {YEAR_MIN}-{YEAR_MAX}\")\n",
    "if len(available) == 0:\n",
    "    raise FileNotFoundError(\"No Ookla parquet files found for 2019–2025.\")\n",
    "\n",
    "# STEP 4: PROCESS & AGGREGATE\n",
    "\n",
    "all_national = []\n",
    "all_subnat = []\n",
    "all_grid_long = []\n",
    "all_grid_long_z16 = []\n",
    "\n",
    "# Group files by period\n",
    "from collections import defaultdict\n",
    "by_period = defaultdict(list)\n",
    "for year, quarter, fp in available:\n",
    "    by_period[(year, quarter)].append(fp)\n",
    "\n",
    "for (year, quarter), file_paths in sorted(by_period.items()):\n",
    "    print(\"\\n\" + \"-\" * 72)\n",
    "    print(f\"Processing {year}-Q{quarter} ({len(file_paths)} file(s))\")\n",
    "    try:\n",
    "        dfs = []\n",
    "        for file_path in file_paths:\n",
    "            dfi = pd.read_parquet(file_path, engine=\"fastparquet\")\n",
    "            dfs.append(dfi)\n",
    "        df = pd.concat(dfs, ignore_index=True)\n",
    "        print(f\"  • Loaded: {len(df):,} rows; columns: {list(df.columns)}\")\n",
    "\n",
    "        # Validate data\n",
    "        if not validate_ookla_data(df, year, quarter):\n",
    "            continue\n",
    "\n",
    "        # Mobile network filter (REQUIRED if column exists; otherwise assume mobile-only source)\n",
    "        if \"network\" in df.columns:\n",
    "            before = len(df)\n",
    "            df = df[df[\"network\"].astype(str).str.lower() == NET_TYPE.lower()].copy()\n",
    "            print(f\"  • Filtered by NET_TYPE='{NET_TYPE}': {before:,} → {len(df):,}\")\n",
    "            if len(df) == 0:\n",
    "                print(f\"  ⚠ No {NET_TYPE} data in this period; skipping.\")\n",
    "                continue\n",
    "        else:\n",
    "            print(f\"  • 'network' column not found; proceeding (source assumed {NET_TYPE}).\")\n",
    "\n",
    "        # Algeria filter\n",
    "        if \"quadkey\" in df.columns and len(QK16_SET) > 0:\n",
    "            df[\"quadkey\"] = df[\"quadkey\"].astype(str)\n",
    "            df_dza = df[df[\"quadkey\"].isin(QK16_SET)].copy()\n",
    "            print(f\"  • Quadkey filter kept: {len(df_dza):,}\")\n",
    "        elif {\"lat\", \"lon\"}.issubset(df.columns):\n",
    "            xmin, ymin, xmax, ymax = ALGERIA_BOUNDS\n",
    "            df_dza = df[(df[\"lat\"] >= ymin) & (df[\"lat\"] <= ymax) &\n",
    "                        (df[\"lon\"] >= xmin) & (df[\"lon\"] <= xmax)].copy()\n",
    "            print(f\"  • BBox filter kept: {len(df_dza):,}\")\n",
    "        else:\n",
    "            print(\"  ✗ No 'quadkey' or 'lat/lon' columns; skipping period.\")\n",
    "            continue\n",
    "\n",
    "        if len(df_dza) == 0:\n",
    "            print(\"  ⚠ No Algeria records in this period; skipping.\")\n",
    "            continue\n",
    "\n",
    "        # NATIONAL STATS\n",
    "        nat = {\n",
    "            \"year\": year,\n",
    "            \"quarter\": quarter,\n",
    "            \"date\": f\"{year}-Q{quarter}\",\n",
    "            \"avg_download_mbps\": (df_dza[\"avg_d_kbps\"].mean() / 1000.0),\n",
    "            \"avg_upload_mbps\": (df_dza[\"avg_u_kbps\"].mean() / 1000.0),\n",
    "            \"median_download_mbps\": (df_dza[\"avg_d_kbps\"].median() / 1000.0),\n",
    "            \"median_upload_mbps\": (df_dza[\"avg_u_kbps\"].median() / 1000.0),\n",
    "            \"avg_latency_ms\": (df_dza[\"avg_lat_ms\"].mean() if \"avg_lat_ms\" in df_dza.columns else None),\n",
    "            \"median_latency_ms\": (df_dza[\"avg_lat_ms\"].median() if \"avg_lat_ms\" in df_dza.columns else None),\n",
    "            \"num_tiles\": len(df_dza),\n",
    "            \"num_tests\": int(df_dza[\"tests\"].sum()) if \"tests\" in df_dza.columns else None,\n",
    "            \"num_devices\": int(df_dza[\"devices\"].sum()) if \"devices\" in df_dza.columns else None,\n",
    "        }\n",
    "        if \"tests\" in df_dza.columns:\n",
    "            w = df_dza[\"tests\"].clip(lower=0)\n",
    "            if w.sum() > 0:\n",
    "                nat[\"wavg_download_mbps\"] = (df_dza[\"avg_d_kbps\"] / 1000.0 * w).sum() / w.sum()\n",
    "                nat[\"wavg_upload_mbps\"] = (df_dza[\"avg_u_kbps\"] / 1000.0 * w).sum() / w.sum()\n",
    "                if \"avg_lat_ms\" in df_dza.columns:\n",
    "                    nat[\"wavg_latency_ms\"] = (df_dza[\"avg_lat_ms\"] * w).sum() / w.sum()\n",
    "        all_national.append(nat)\n",
    "        print(f\"  ✓ National mean: {nat['avg_download_mbps']:.2f}↓ / {nat['avg_upload_mbps']:.2f}↑ Mbps\")\n",
    "\n",
    "        # SUBNATIONAL\n",
    "        gdf_poly = ensure_tile_polygon_gdf(df_dza)\n",
    "\n",
    "        adm1_stats = aggregate_by_admin(gdf_poly, boundary_adm1, \"shapeISO\", \"shapeName\", \"ADM1\")\n",
    "        adm1_stats[\"year\"] = year; adm1_stats[\"quarter\"] = quarter; adm1_stats[\"date\"] = f\"{year}-Q{quarter}\"\n",
    "        all_subnat.append(adm1_stats); print(f\"  ✓ ADM1 aggregated: {len(adm1_stats)} rows\")\n",
    "\n",
    "        adm2_stats = aggregate_by_admin(gdf_poly, boundary_adm2, \"shapeISO\", \"shapeName\", \"ADM2\")\n",
    "        adm2_stats[\"year\"] = year; adm2_stats[\"quarter\"] = quarter; adm2_stats[\"date\"] = f\"{year}-Q{quarter}\"\n",
    "        all_subnat.append(adm2_stats); print(f\"  ✓ ADM2 aggregated: {len(adm2_stats)} rows\")\n",
    "\n",
    "        adm3_stats = aggregate_by_admin(gdf_poly, boundary_adm3, \"shapeISO\", \"shapeName\", \"ADM3\")\n",
    "        adm3_stats[\"year\"] = year; adm3_stats[\"quarter\"] = quarter; adm3_stats[\"date\"] = f\"{year}-Q{quarter}\"\n",
    "        all_subnat.append(adm3_stats); print(f\"  ✓ ADM3 aggregated: {len(adm3_stats)} rows\")\n",
    "\n",
    "        # GRID z12\n",
    "        if \"quadkey\" in df_dza.columns:\n",
    "            df_dza[\"quadkey_z12\"] = df_dza[\"quadkey\"].astype(str).str[:12]\n",
    "            agg_cols = {\n",
    "                \"avg_d_kbps\": \"mean\",\n",
    "                \"avg_u_kbps\": \"mean\",\n",
    "            }\n",
    "            if \"avg_lat_ms\" in df_dza.columns:\n",
    "                agg_cols[\"avg_lat_ms\"] = \"mean\"\n",
    "            if \"tests\" in df_dza.columns:\n",
    "                agg_cols[\"tests\"] = \"sum\"\n",
    "\n",
    "            grid_stats = df_dza.groupby(\"quadkey_z12\").agg(agg_cols).reset_index()\n",
    "            grid_stats[\"year\"] = year\n",
    "            grid_stats[\"quarter\"] = quarter\n",
    "            grid_stats[\"date\"] = f\"{year}-Q{quarter}\"\n",
    "            grid_stats[\"avg_download_mbps\"] = grid_stats[\"avg_d_kbps\"] / 1000.0\n",
    "            grid_stats[\"avg_upload_mbps\"] = grid_stats[\"avg_u_kbps\"] / 1000.0\n",
    "\n",
    "            keep_cols = [\"quadkey_z12\", \"year\", \"quarter\", \"date\",\n",
    "                         \"avg_download_mbps\", \"avg_upload_mbps\"]\n",
    "            if \"avg_lat_ms\" in grid_stats.columns:\n",
    "                keep_cols.append(\"avg_lat_ms\")\n",
    "            if \"tests\" in df_dza.columns:\n",
    "                keep_cols.append(\"tests\")\n",
    "\n",
    "            all_grid_long.append(grid_stats[keep_cols])\n",
    "            print(f\"  ✓ Grid z12 long rows added: {len(grid_stats)}\")\n",
    "\n",
    "        # GRID z16\n",
    "        if \"quadkey\" in df_dza.columns:\n",
    "            agg_cols16 = {\"avg_d_kbps\": \"mean\", \"avg_u_kbps\": \"mean\"}\n",
    "            if \"avg_lat_ms\" in df_dza.columns:\n",
    "                agg_cols16[\"avg_lat_ms\"] = \"mean\"\n",
    "            if \"tests\" in df_dza.columns:\n",
    "                agg_cols16[\"tests\"] = \"sum\"\n",
    "\n",
    "            grid_stats_z16 = (\n",
    "                df_dza.groupby(df_dza[\"quadkey\"].astype(str))\n",
    "                .agg(agg_cols16)\n",
    "                .reset_index()\n",
    "                .rename(columns={\"quadkey\": \"quadkey_z16\"})\n",
    "            )\n",
    "            grid_stats_z16[\"year\"] = year\n",
    "            grid_stats_z16[\"quarter\"] = quarter\n",
    "            grid_stats_z16[\"date\"] = f\"{year}-Q{quarter}\"\n",
    "            grid_stats_z16[\"avg_download_mbps\"] = grid_stats_z16[\"avg_d_kbps\"] / 1000.0\n",
    "            grid_stats_z16[\"avg_upload_mbps\"] = grid_stats_z16[\"avg_u_kbps\"] / 1000.0\n",
    "\n",
    "            keep_z16 = [\"quadkey_z16\", \"year\", \"quarter\", \"date\",\n",
    "                        \"avg_download_mbps\", \"avg_upload_mbps\"]\n",
    "            if \"avg_lat_ms\" in grid_stats_z16.columns:\n",
    "                keep_z16.append(\"avg_lat_ms\")\n",
    "            if \"tests\" in df_dza.columns:\n",
    "                keep_z16.append(\"tests\")\n",
    "\n",
    "            all_grid_long_z16.append(grid_stats_z16[keep_z16])\n",
    "            print(f\"  ✓ Grid z16 long rows added: {len(grid_stats_z16)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error processing {year}-Q{quarter}: {e}\")\n",
    "        import traceback; traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "# STEP 5: SAVE OUTPUTS (UTF-8)\n",
    "\n",
    "# National trends\n",
    "if not all_national:\n",
    "    raise ValueError(\"No national data processed; aborting save.\")\n",
    "df_national = pd.DataFrame(all_national).sort_values([\"year\", \"quarter\"]).reset_index(drop=True)\n",
    "df_national.to_csv(os.path.join(OUTPUT_DIR, \"algeria_national_trends_mobile.csv\"), index=False, encoding=\"utf-8\")\n",
    "print(f\"  ✓ National trends: {len(df_national)} periods → {OUTPUT_DIR}algeria_national_trends_mobile.csv\")\n",
    "\n",
    "# Subnational trends\n",
    "if all_subnat:\n",
    "    df_sub = pd.concat(all_subnat, ignore_index=True)\n",
    "    df_sub = df_sub.sort_values([\"admin_level\", \"admin_name\", \"year\", \"quarter\"]).reset_index(drop=True)\n",
    "    df_sub.to_csv(os.path.join(OUTPUT_DIR, \"algeria_subnational_trends_mobile.csv\"), index=False, encoding=\"utf-8\")\n",
    "    print(f\"  ✓ Subnational trends: {len(df_sub)} rows → {OUTPUT_DIR}algeria_subnational_trends_mobile.csv\")\n",
    "else:\n",
    "    df_sub = pd.DataFrame()\n",
    "    print(\"  ⚠ No subnational aggregates to save.\")\n",
    "\n",
    "# Grid long (z12)\n",
    "if all_grid_long:\n",
    "    df_grid_long = pd.concat(all_grid_long, ignore_index=True)\n",
    "    df_grid_long.to_csv(os.path.join(OUTPUT_DIR, \"algeria_grid_data_long_z12_mobile.csv\"), index=False, encoding=\"utf-8\")\n",
    "    print(f\"  ✓ Grid long z12: {len(df_grid_long)} rows → {OUTPUT_DIR}algeria_grid_data_long_z12_mobile.csv\")\n",
    "\n",
    "    # z12 wide CSV with WKT geometry\n",
    "    print(\"\\n[STEP 5B] Building z12 wide GeoParquet for mapping...\")\n",
    "    dl_wide = df_grid_long.pivot(index=\"quadkey_z12\", columns=\"date\", values=\"avg_download_mbps\")\n",
    "    ul_wide = df_grid_long.pivot(index=\"quadkey_z12\", columns=\"date\", values=\"avg_upload_mbps\")\n",
    "    dl_wide.columns = [f\"download_{c}\" for c in dl_wide.columns]\n",
    "    ul_wide.columns = [f\"upload_{c}\" for c in ul_wide.columns]\n",
    "\n",
    "    parts = [dl_wide, ul_wide]\n",
    "    if \"avg_lat_ms\" in df_grid_long.columns:\n",
    "        lat_wide = df_grid_long.pivot(index=\"quadkey_z12\", columns=\"date\", values=\"avg_lat_ms\")\n",
    "        lat_wide.columns = [f\"latency_{c}\" for c in lat_wide.columns]\n",
    "        parts.append(lat_wide)\n",
    "\n",
    "    grid_wide = pd.concat(parts, axis=1)\n",
    "    grid_wide[\"geometry\"] = grid_wide.index.map(quadkey_to_polygon)\n",
    "    gdf_grid_wide = gpd.GeoDataFrame(grid_wide, crs=\"EPSG:4326\").reset_index(names=\"quadkey_z12\")\n",
    "    _gw = gdf_grid_wide.copy()\n",
    "    _gw[\"wkt\"] = _gw.geometry.apply(lambda g: g.wkt)\n",
    "    _gw.drop(columns=\"geometry\").to_csv(os.path.join(OUTPUT_DIR, \"algeria_grid_timeseries_mobile_wkt.csv\"),\n",
    "                                        index=False, encoding=\"utf-8\")\n",
    "    print(f\"  ✓ z12 time-series GeoParquet → {OUTPUT_DIR}algeria_grid_timeseries_mobile_wkt.csv\")\n",
    "else:\n",
    "    print(\"  ⚠ No grid-long z12 data; skipped z12 wide/GeoPackage build.\")\n",
    "\n",
    "# Grid long (z16)\n",
    "if all_grid_long_z16:\n",
    "    df_grid_long_z16 = pd.concat(all_grid_long_z16, ignore_index=True)\n",
    "    df_grid_long_z16.to_csv(os.path.join(OUTPUT_DIR, \"algeria_grid_data_long_z16_mobile.csv\"),\n",
    "                            index=False, encoding=\"utf-8\")\n",
    "    print(f\"  ✓ Grid long z16: {len(df_grid_long_z16)} rows → {OUTPUT_DIR}algeria_grid_data_long_z16_mobile.csv\")\n",
    "\n",
    "print(\"PREPROCESSING COMPLETE\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algeria_economic_monitoring",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
