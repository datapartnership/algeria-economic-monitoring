{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, glob, json\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import mercantile\n",
    "from shapely.geometry import Point, box, shape\n",
    "from shapely import wkt\n",
    "from tqdm import tqdm\n",
    "import rasterio\n",
    "from rasterstats import zonal_stats\n",
    "\n",
    "# CONFIG\n",
    "PATH_DATA = \"/ookla_algeria_fixed/\"  \n",
    "OUTPUT_DIR = \"processed_data/\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "NET_TYPE = \"fixed\"\n",
    "ISO_CODE = \"DZA\"\n",
    "YEAR_MIN, YEAR_MAX = 2019, 2025\n",
    "YEARS = list(range(YEAR_MIN, YEAR_MAX + 1))\n",
    "\n",
    "# Boundaries\n",
    "PATH_ADM0 = \"geoBoundaries-DZA-ADM0.geojson\"\n",
    "PATH_ADM1 = \"geoBoundaries-DZA-ADM1.geojson\"\n",
    "PATH_ADM3 = \"geoBoundaries-DZA-ADM3.geojson\"\n",
    "\n",
    "# WorldPop\n",
    "WORLDPOP_DIR = \"worldpop_tifs/\"\n",
    "CHUNK_SIZE = 2500        \n",
    "GDAL_CACHE = 128         \n",
    "\n",
    "print(\"ALGERIA OOKLA DATA PREPROCESSING PIPELINE\")\n",
    "\n",
    "# UTILITIES\n",
    "def tile_to_quadkey(x: int, y: int, z: int) -> str:\n",
    "    qk = \"\"\n",
    "    for i in range(z, 0, -1):\n",
    "        digit, mask = 0, 1 << (i - 1)\n",
    "        if (x & mask) != 0:\n",
    "            digit += 1\n",
    "        if (y & mask) != 0:\n",
    "            digit += 2\n",
    "        qk += str(digit)\n",
    "    return qk\n",
    "\n",
    "\n",
    "def quadkey_to_point(quadkey: str):\n",
    "    t = mercantile.quadkey_to_tile(quadkey)\n",
    "    b = mercantile.bounds(t)\n",
    "    return Point((b.west + b.east) / 2, (b.south + b.north) / 2)\n",
    "\n",
    "\n",
    "def quadkey_to_polygon(quadkey: str):\n",
    "    t = mercantile.quadkey_to_tile(quadkey)\n",
    "    b = mercantile.bounds(t)\n",
    "    return box(b.west, b.south, b.east, b.north)\n",
    "\n",
    "\n",
    "def get_country_quadkeys_at_zoom(boundary_gdf: gpd.GeoDataFrame, zoom_level: int) -> list:\n",
    "    minx, miny, maxx, maxy = boundary_gdf.total_bounds\n",
    "    tiles = list(mercantile.tiles(minx, miny, maxx, maxy, zoom_level))\n",
    "    quadkeys, pts = [], []\n",
    "    for t in tqdm(tiles, desc=f\"Processing zoom {zoom_level} tiles\"):\n",
    "        qk = tile_to_quadkey(t.x, t.y, t.z)\n",
    "        quadkeys.append(qk)\n",
    "        pts.append(quadkey_to_point(qk))\n",
    "    gdf = gpd.GeoDataFrame({\"quadkey\": quadkeys, \"geometry\": pts}, crs=\"EPSG:4326\")\n",
    "    gdf_f = gpd.sjoin(gdf, boundary_gdf[[\"geometry\"]], how=\"inner\", predicate=\"within\")\n",
    "    return gdf_f[\"quadkey\"].astype(str).tolist()\n",
    "\n",
    "\n",
    "def get_subquadkeys(parent_quadkey: str, target_zoom: int) -> list:\n",
    "    current_zoom = len(parent_quadkey)\n",
    "    delta = target_zoom - current_zoom\n",
    "    if delta < 0:\n",
    "        raise ValueError(\"Target zoom must be >= parent zoom\")\n",
    "    if delta == 0:\n",
    "        return [parent_quadkey]\n",
    "    out = []\n",
    "    for suffix in product(\"0123\", repeat=delta):\n",
    "        out.append(parent_quadkey + \"\".join(suffix))\n",
    "    return out\n",
    "\n",
    "\n",
    "def ensure_tile_polygon_gdf(df: pd.DataFrame) -> gpd.GeoDataFrame:\n",
    "    if \"quadkey\" in df.columns:\n",
    "        df = df.copy()\n",
    "        df[\"geometry\"] = df[\"quadkey\"].astype(str).apply(quadkey_to_polygon)\n",
    "    elif {\"lat\", \"lon\"}.issubset(df.columns):\n",
    "        df = df.copy()\n",
    "        df[\"geometry\"] = [Point(xy) for xy in zip(df[\"lon\"], df[\"lat\"])]\n",
    "    else:\n",
    "        raise ValueError(\"No geometry source (quadkey or lat/lon) found.\")\n",
    "    return gpd.GeoDataFrame(df, crs=\"EPSG:4326\")\n",
    "\n",
    "\n",
    "def _flatten_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    new_cols = []\n",
    "    for col in df.columns:\n",
    "        if isinstance(col, tuple):\n",
    "            a, b = col\n",
    "            new_cols.append(f\"{a}_{b}\" if b else a)\n",
    "        else:\n",
    "            new_cols.append(col)\n",
    "    df.columns = new_cols\n",
    "    return df\n",
    "\n",
    "\n",
    "def validate_ookla_data(df: pd.DataFrame, year: int, quarter: int) -> bool:\n",
    "    required_cols = [\"avg_d_kbps\", \"avg_u_kbps\"]\n",
    "    missing = [c for c in required_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"{year}-Q{quarter}: Missing required columns: {missing}\")\n",
    "    if len(df) == 0:\n",
    "        print(f\"Warning: {year}-Q{quarter} has no rows\")\n",
    "        return False\n",
    "    neg_down = (df[\"avg_d_kbps\"] < 0).sum()\n",
    "    neg_up = (df[\"avg_u_kbps\"] < 0).sum()\n",
    "    if neg_down > 0 or neg_up > 0:\n",
    "        print(f\"Warning: {year}-Q{quarter} has {neg_down} negative download and {neg_up} negative upload values\")\n",
    "    extreme_down = (df[\"avg_d_kbps\"] > 1_000_000).sum()\n",
    "    extreme_up = (df[\"avg_u_kbps\"] > 1_000_000).sum()\n",
    "    if extreme_down > 0 or extreme_up > 0:\n",
    "        print(f\"Warning: {year}-Q{quarter} has {extreme_down} extreme download and {extreme_up} extreme upload values (>1 Gbps)\")\n",
    "    if \"avg_lat_ms\" in df.columns:\n",
    "        neg_lat = (df[\"avg_lat_ms\"] < 0).sum()\n",
    "        extreme_lat = (df[\"avg_lat_ms\"] > 1000).sum()\n",
    "        if neg_lat > 0:\n",
    "            print(f\"Warning: {year}-Q{quarter} has {neg_lat} negative latency values\")\n",
    "        if extreme_lat > 0:\n",
    "            print(f\"Warning: {year}-Q{quarter} has {extreme_lat} extreme latency values (>1000ms)\")\n",
    "    return True\n",
    "\n",
    "\n",
    "def validate_boundaries(boundary_gdf: gpd.GeoDataFrame, level: str, required_cols: list) -> bool:\n",
    "    missing = [c for c in required_cols if c not in boundary_gdf.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"{level}: Missing required columns: {missing}\")\n",
    "    invalid = (~boundary_gdf.geometry.is_valid).sum()\n",
    "    if invalid > 0:\n",
    "        print(f\"Warning: {level} has {invalid} invalid geometries (will be fixed with buffer(0))\")\n",
    "    return True\n",
    "\n",
    "\n",
    "def read_geojson_no_gdal(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        gj = json.load(f)\n",
    "    recs = []\n",
    "    for feat in gj[\"features\"]:\n",
    "        props = feat.get(\"properties\", {})\n",
    "        geom = shape(feat[\"geometry\"])\n",
    "        recs.append({**props, \"geometry\": geom})\n",
    "    return gpd.GeoDataFrame(recs, crs=\"EPSG:4326\")\n",
    "\n",
    "\n",
    "def aggregate_by_admin(gdf_meas: gpd.GeoDataFrame, admin_gdf: gpd.GeoDataFrame,\n",
    "                       id_col: str, name_col: str, admin_level_tag: str) -> pd.DataFrame:\n",
    "    joined = gpd.sjoin(gdf_meas, admin_gdf[[id_col, name_col, \"geometry\"]],\n",
    "                       how=\"left\", predicate=\"intersects\")\n",
    "    has_tests = \"tests\" in joined.columns\n",
    "    has_devices = \"devices\" in joined.columns\n",
    "    has_latency = \"avg_lat_ms\" in joined.columns\n",
    "\n",
    "    agg_dict = {\n",
    "        \"avg_d_kbps\": [\"mean\", \"median\", \"count\"],\n",
    "        \"avg_u_kbps\": [\"mean\", \"median\"],\n",
    "    }\n",
    "    if has_latency:\n",
    "        agg_dict[\"avg_lat_ms\"] = [\"mean\", \"median\"]\n",
    "    if has_tests:\n",
    "        agg_dict[\"tests\"] = \"sum\"\n",
    "    if has_devices:\n",
    "        agg_dict[\"devices\"] = \"sum\"\n",
    "\n",
    "    out = joined.groupby([id_col, name_col]).agg(agg_dict).reset_index()\n",
    "    out = _flatten_columns(out)\n",
    "    rename_map = {\n",
    "        \"avg_d_kbps_mean\": \"avg_download_kbps\",\n",
    "        \"avg_d_kbps_median\": \"median_download_kbps\",\n",
    "        \"avg_d_kbps_count\": \"num_tiles\",\n",
    "        \"avg_u_kbps_mean\": \"avg_upload_kbps\",\n",
    "        \"avg_u_kbps_median\": \"median_upload_kbps\",\n",
    "        \"tests_sum\": \"num_tests\",\n",
    "        \"devices_sum\": \"num_devices\",\n",
    "        \"avg_lat_ms_mean\": \"avg_latency_ms\",\n",
    "        \"avg_lat_ms_median\": \"median_latency_ms\",\n",
    "    }\n",
    "    out = out.rename(columns=rename_map)\n",
    "    out[\"admin_level\"] = admin_level_tag\n",
    "    out[\"admin_code\"] = out[id_col].astype(str)\n",
    "    out[\"admin_name\"] = out[name_col].astype(str)\n",
    "\n",
    "    if \"avg_download_kbps\" in out.columns:\n",
    "        out[\"avg_download_mbps\"] = out[\"avg_download_kbps\"] / 1000.0\n",
    "    if \"avg_upload_kbps\" in out.columns:\n",
    "        out[\"avg_upload_mbps\"] = out[\"avg_upload_kbps\"] / 1000.0\n",
    "    if \"median_download_kbps\" in out.columns:\n",
    "        out[\"median_download_mbps\"] = out[\"median_download_kbps\"] / 1000.0\n",
    "    if \"median_upload_kbps\" in out.columns:\n",
    "        out[\"median_upload_mbps\"] = out[\"median_upload_kbps\"] / 1000.0\n",
    "\n",
    "    keep = [\"admin_level\", \"admin_code\", \"admin_name\",\n",
    "            \"avg_download_mbps\", \"avg_upload_mbps\",\n",
    "            \"median_download_mbps\", \"median_upload_mbps\", \"num_tiles\"]\n",
    "    if \"avg_latency_ms\" in out.columns:\n",
    "        keep += [\"avg_latency_ms\", \"median_latency_ms\"]\n",
    "    if \"num_tests\" in out.columns:\n",
    "        keep.append(\"num_tests\")\n",
    "    if \"num_devices\" in out.columns:\n",
    "        keep.append(\"num_devices\")\n",
    "    return out[keep]\n",
    "\n",
    "\n",
    "def infer_year(path):\n",
    "    m = re.search(r\"(20\\d{2})\", os.path.basename(path))\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "\n",
    "# STEP 1: LOAD BOUNDARIES\n",
    "for pth in [PATH_ADM0, PATH_ADM1, PATH_ADM3]:\n",
    "    if not os.path.exists(pth):\n",
    "        raise FileNotFoundError(pth)\n",
    "\n",
    "boundary_national = read_geojson_no_gdal(PATH_ADM0).to_crs(\"EPSG:4326\")\n",
    "print(\"Loaded national boundary (ADM0)\")\n",
    "\n",
    "boundary_adm1 = read_geojson_no_gdal(PATH_ADM1)[[\"shapeISO\", \"shapeName\", \"geometry\"]].to_crs(\"EPSG:4326\")\n",
    "boundary_adm1[\"shapeName\"] = boundary_adm1[\"shapeName\"].astype(str)\n",
    "validate_boundaries(boundary_adm1, \"ADM1\", [\"shapeISO\", \"shapeName\", \"geometry\"])\n",
    "print(f\"Loaded ADM1: {len(boundary_adm1)} wilayas\")\n",
    "\n",
    "boundary_adm3 = read_geojson_no_gdal(PATH_ADM3)[[\"shapeISO\", \"shapeName\", \"geometry\"]].to_crs(\"EPSG:4326\")\n",
    "boundary_adm3[\"shapeName\"] = boundary_adm3[\"shapeName\"].astype(str)\n",
    "validate_boundaries(boundary_adm3, \"ADM3\", [\"shapeISO\", \"shapeName\", \"geometry\"])\n",
    "print(f\"Loaded ADM3: {len(boundary_adm3)} communes\")\n",
    "\n",
    "for gdf_fix in [boundary_national, boundary_adm1, boundary_adm3]:\n",
    "    if isinstance(gdf_fix, gpd.GeoDataFrame):\n",
    "        gdf_fix.geometry = gdf_fix.buffer(0)\n",
    "\n",
    "ALGERIA_BOUNDS = boundary_national.total_bounds\n",
    "\n",
    "# STEP 2: QUADKEY FILTERS AND Z12 GRID\n",
    "print(\"STEP 2: Generating quadkey filter (z10 to z16) and z12 grid\")\n",
    "\n",
    "parent_qk_z10 = get_country_quadkeys_at_zoom(boundary_national, zoom_level=10)\n",
    "\n",
    "print(\"Expanding to zoom 16 quadkeys (Ookla native resolution)\")\n",
    "all_qk_z16 = []\n",
    "for pqk in tqdm(parent_qk_z10, desc=\"Expanding quadkeys\"):\n",
    "    all_qk_z16.extend(get_subquadkeys(pqk, 16))\n",
    "print(f\"Generated {len(all_qk_z16):,} z16 quadkeys for Algeria\")\n",
    "\n",
    "pd.DataFrame({\"quadkey\": all_qk_z16, \"country\": ISO_CODE}).to_csv(\n",
    "    os.path.join(OUTPUT_DIR, \"algeria_quadkeys_z16.csv\"), index=False, encoding=\"utf-8\"\n",
    ")\n",
    "QK16_SET = set(all_qk_z16)\n",
    "\n",
    "print(\"Creating z12 grid (polygons) for mapping\")\n",
    "parent_qk_z12 = get_country_quadkeys_at_zoom(boundary_national, zoom_level=12)\n",
    "gdf_grid_z12 = gpd.GeoDataFrame(\n",
    "    {\"quadkey\": parent_qk_z12, \"geometry\": [quadkey_to_polygon(qk) for qk in parent_qk_z12]},\n",
    "    crs=\"EPSG:4326\",\n",
    ")\n",
    "gdf_grid_z12[\"quadkey\"] = gdf_grid_z12[\"quadkey\"].astype(str)\n",
    "print(f\"Created {len(gdf_grid_z12)} z12 tiles\")\n",
    "\n",
    "_gz12 = gdf_grid_z12.copy()\n",
    "_gz12[\"quadkey\"] = _gz12[\"quadkey\"].astype(str)\n",
    "_gz12[\"wkt\"] = _gz12.geometry.apply(lambda g: g.wkt)\n",
    "_gz12.drop(columns=\"geometry\").to_csv(\n",
    "    os.path.join(OUTPUT_DIR, \"algeria_grid_z12_wkt.csv\"), index=False, encoding=\"utf-8\"\n",
    ")\n",
    "print(f\"Saved grid WKT: {os.path.join(OUTPUT_DIR, 'algeria_grid_z12_wkt.csv')}\")\n",
    "\n",
    "print(\"Building tile to admin (ADM1 and ADM3) lookup\")\n",
    "gdf_grid_z12_centroids = gdf_grid_z12.copy()\n",
    "gdf_grid_z12_centroids = gdf_grid_z12_centroids.to_crs(3857)\n",
    "gdf_grid_z12_centroids[\"geometry\"] = gdf_grid_z12_centroids.geometry.centroid\n",
    "gdf_grid_z12_centroids = gdf_grid_z12_centroids.to_crs(4326)\n",
    "\n",
    "adm1_ren = boundary_adm1.rename(columns={\"shapeISO\": \"adm1_code\", \"shapeName\": \"adm1_name\"})\n",
    "adm3_ren = boundary_adm3.rename(columns={\"shapeISO\": \"adm3_code\", \"shapeName\": \"adm3_name\"})\n",
    "\n",
    "lk = gpd.sjoin(\n",
    "    gdf_grid_z12_centroids[[\"quadkey\", \"geometry\"]],\n",
    "    adm1_ren[[\"adm1_code\", \"adm1_name\", \"geometry\"]],\n",
    "    how=\"left\", predicate=\"within\"\n",
    ").drop(columns=\"index_right\")\n",
    "lk = gpd.sjoin(\n",
    "    lk,\n",
    "    adm3_ren[[\"adm3_code\", \"adm3_name\", \"geometry\"]],\n",
    "    how=\"left\", predicate=\"within\"\n",
    ").drop(columns=[\"index_right\", \"geometry\"])\n",
    "\n",
    "lk[\"quadkey_z12\"] = lk[\"quadkey\"].astype(str)\n",
    "lookup = lk[[\"quadkey_z12\", \"adm1_code\", \"adm1_name\", \"adm3_code\", \"adm3_name\"]]\n",
    "lookup.to_csv(os.path.join(OUTPUT_DIR, \"tile_admin_lookup_z12.csv\"), index=False, encoding=\"utf-8\")\n",
    "print(f\"Saved: {os.path.join(OUTPUT_DIR, 'tile_admin_lookup_z12.csv')}\")\n",
    "\n",
    "# STEP 3: DISCOVER PARQUET FILES\n",
    "available = []\n",
    "if not os.path.isdir(PATH_DATA):\n",
    "    raise FileNotFoundError(f\"Data directory not found: {PATH_DATA}\")\n",
    "\n",
    "for year_folder in sorted(d for d in os.listdir(PATH_DATA) if os.path.isdir(os.path.join(PATH_DATA, d))):\n",
    "    if not year_folder.startswith(\"year=\"):\n",
    "        continue\n",
    "    m_year = re.search(r'year\\s*=\\s*(\\d+)', year_folder)\n",
    "    year = int(m_year.group(1)) if m_year else None\n",
    "    if year is None or not (YEAR_MIN <= year <= YEAR_MAX):\n",
    "        continue\n",
    "    year_path = os.path.join(PATH_DATA, year_folder)\n",
    "    for quarter_folder in sorted(d for d in os.listdir(year_path) if os.path.isdir(os.path.join(year_path, d))):\n",
    "        if not quarter_folder.startswith(\"quarter=\"):\n",
    "            continue\n",
    "        m_quarter = re.search(r'quarter\\s*=\\s*(\\d+)', quarter_folder)\n",
    "        quarter = int(m_quarter.group(1)) if m_quarter else None\n",
    "        if quarter is None:\n",
    "            continue\n",
    "        quarter_path = os.path.join(year_path, quarter_folder)\n",
    "        files = sorted(glob.glob(os.path.join(quarter_path, \"*.parquet\")))\n",
    "        for fp in files:\n",
    "            available.append((year, quarter, fp))\n",
    "\n",
    "print(f\"Found {len(available)} parquet file(s) within {YEAR_MIN}-{YEAR_MAX}\")\n",
    "\n",
    "# STEP 4: PROCESS PARQUETS\n",
    "all_national = []\n",
    "all_subnat = []\n",
    "all_grid_long = []\n",
    "all_grid_long_z16 = []\n",
    "\n",
    "from collections import defaultdict\n",
    "by_period = defaultdict(list)\n",
    "for year, quarter, fp in available:\n",
    "    by_period[(year, quarter)].append(fp)\n",
    "\n",
    "for (year, quarter), file_paths in sorted(by_period.items()):\n",
    "    print(f\"Processing {year}-Q{quarter} ({len(file_paths)} file(s))\")\n",
    "    try:\n",
    "        dfs = [pd.read_parquet(fp, engine=\"fastparquet\") for fp in file_paths]\n",
    "        df = pd.concat(dfs, ignore_index=True)\n",
    "        print(f\"Loaded {len(df):,} rows; columns: {list(df.columns)}\")\n",
    "\n",
    "        if not validate_ookla_data(df, year, quarter):\n",
    "            continue\n",
    "\n",
    "        if \"network\" in df.columns:\n",
    "            before = len(df)\n",
    "            df = df[df[\"network\"].astype(str).str.lower() == NET_TYPE.lower()].copy()\n",
    "            print(f\"Filtered by NET_TYPE='{NET_TYPE}': {before:,} to {len(df):,}\")\n",
    "            if len(df) == 0:\n",
    "                print(f\"No {NET_TYPE} data in this period; skipping\")\n",
    "                continue\n",
    "        else:\n",
    "            print(\"Network column not found; proceeding (assume fixed).\")\n",
    "\n",
    "        if \"quadkey\" in df.columns and len(QK16_SET) > 0:\n",
    "            df[\"quadkey\"] = df[\"quadkey\"].astype(str)\n",
    "            df_dza = df[df[\"quadkey\"].isin(QK16_SET)].copy()\n",
    "            print(f\"Quadkey filter kept {len(df_dza):,} rows\")\n",
    "        elif {\"lat\", \"lon\"}.issubset(df.columns):\n",
    "            xmin, ymin, xmax, ymax = ALGERIA_BOUNDS\n",
    "            df_dza = df[(df[\"lat\"] >= ymin) & (df[\"lat\"] <= ymax) & (df[\"lon\"] >= xmin) & (df[\"lon\"] <= xmax)].copy()\n",
    "            print(f\"Bounding box filter kept {len(df_dza):,} rows\")\n",
    "        else:\n",
    "            print(\"No quadkey or lat/lon columns; skipping.\")\n",
    "            continue\n",
    "\n",
    "        if len(df_dza) == 0:\n",
    "            print(\"No Algeria records; skipping.\")\n",
    "            continue\n",
    "\n",
    "        # National (unweighted)\n",
    "        nat = {\n",
    "            \"year\": year, \"quarter\": quarter, \"date\": f\"{year}-Q{quarter}\",\n",
    "            \"avg_download_mbps\": (df_dza[\"avg_d_kbps\"].mean() / 1000.0),\n",
    "            \"avg_upload_mbps\": (df_dza[\"avg_u_kbps\"].mean() / 1000.0),\n",
    "            \"median_download_mbps\": (df_dza[\"avg_d_kbps\"].median() / 1000.0),\n",
    "            \"median_upload_mbps\": (df_dza[\"avg_u_kbps\"].median() / 1000.0),\n",
    "            \"avg_latency_ms\": (df_dza[\"avg_lat_ms\"].mean() if \"avg_lat_ms\" in df_dza.columns else None),\n",
    "            \"median_latency_ms\": (df_dza[\"avg_lat_ms\"].median() if \"avg_lat_ms\" in df_dza.columns else None),\n",
    "            \"num_tiles\": len(df_dza),\n",
    "            \"num_tests\": int(df_dza[\"tests\"].sum()) if \"tests\" in df_dza.columns else None,\n",
    "            \"num_devices\": int(df_dza[\"devices\"].sum()) if \"devices\" in df_dza.columns else None,\n",
    "        }\n",
    "        if \"tests\" in df_dza.columns:\n",
    "            w = df_dza[\"tests\"].clip(lower=0)\n",
    "            if w.sum() > 0:\n",
    "                nat[\"wavg_download_mbps\"] = (df_dza[\"avg_d_kbps\"] / 1000.0 * w).sum() / w.sum()\n",
    "                nat[\"wavg_upload_mbps\"] = (df_dza[\"avg_u_kbps\"] / 1000.0 * w).sum() / w.sum()\n",
    "                if \"avg_lat_ms\" in df_dza.columns:\n",
    "                    nat[\"wavg_latency_ms\"] = (df_dza[\"avg_lat_ms\"] * w).sum() / w.sum()\n",
    "        all_national.append(nat)\n",
    "        print(f\"National mean: {nat['avg_download_mbps']:.2f} down / {nat['avg_upload_mbps']:.2f} up Mbps\")\n",
    "\n",
    "        # Subnational (unweighted)\n",
    "        gdf_poly = ensure_tile_polygon_gdf(df_dza)\n",
    "        adm1_stats = aggregate_by_admin(gdf_poly, boundary_adm1, \"shapeISO\", \"shapeName\", \"ADM1\")\n",
    "        adm1_stats[\"year\"] = year\n",
    "        adm1_stats[\"quarter\"] = quarter\n",
    "        adm1_stats[\"date\"] = f\"{year}-Q{quarter}\"\n",
    "        all_subnat.append(adm1_stats)\n",
    "        print(f\"ADM1 aggregated: {len(adm1_stats)} rows\")\n",
    "\n",
    "        adm3_stats = aggregate_by_admin(gdf_poly, boundary_adm3, \"shapeISO\", \"shapeName\", \"ADM3\")\n",
    "        adm3_stats[\"year\"] = year\n",
    "        adm3_stats[\"quarter\"] = quarter\n",
    "        adm3_stats[\"date\"] = f\"{year}-Q{quarter}\"\n",
    "        all_subnat.append(adm3_stats)\n",
    "        print(f\"ADM3 aggregated: {len(adm3_stats)} rows\")\n",
    "\n",
    "        # Grid z12 long\n",
    "        if \"quadkey\" in df_dza.columns:\n",
    "            df_dza[\"quadkey_z12\"] = df_dza[\"quadkey\"].astype(str).str[:12]\n",
    "            agg_cols = {\"avg_d_kbps\": \"mean\", \"avg_u_kbps\": \"mean\"}\n",
    "            if \"avg_lat_ms\" in df_dza.columns:\n",
    "                agg_cols[\"avg_lat_ms\"] = \"mean\"\n",
    "            if \"tests\" in df_dza.columns:\n",
    "                agg_cols[\"tests\"] = \"sum\"\n",
    "\n",
    "            grid_stats = df_dza.groupby(\"quadkey_z12\").agg(agg_cols).reset_index()\n",
    "            grid_stats[\"year\"] = year\n",
    "            grid_stats[\"quarter\"] = quarter\n",
    "            grid_stats[\"date\"] = f\"{year}-Q{quarter}\"\n",
    "            grid_stats[\"avg_download_mbps\"] = grid_stats[\"avg_d_kbps\"] / 1000.0\n",
    "            grid_stats[\"avg_upload_mbps\"] = grid_stats[\"avg_u_kbps\"] / 1000.0\n",
    "\n",
    "            keep_cols = [\"quadkey_z12\", \"year\", \"quarter\", \"date\", \"avg_download_mbps\", \"avg_upload_mbps\"]\n",
    "            if \"avg_lat_ms\" in grid_stats.columns:\n",
    "                keep_cols.append(\"avg_lat_ms\")\n",
    "            if \"tests\" in df_dza.columns:\n",
    "                keep_cols.append(\"tests\")\n",
    "\n",
    "            all_grid_long.append(grid_stats[keep_cols])\n",
    "            print(f\"Grid z12 long rows added: {len(grid_stats)}\")\n",
    "\n",
    "        # Grid z16 long\n",
    "        if \"quadkey\" in df_dza.columns:\n",
    "            agg_cols16 = {\"avg_d_kbps\": \"mean\", \"avg_u_kbps\": \"mean\"}\n",
    "            if \"avg_lat_ms\" in df_dza.columns:\n",
    "                agg_cols16[\"avg_lat_ms\"] = \"mean\"\n",
    "            if \"tests\" in df_dza.columns:\n",
    "                agg_cols16[\"tests\"] = \"sum\"\n",
    "\n",
    "            grid_stats_z16 = (\n",
    "                df_dza.groupby(df_dza[\"quadkey\"].astype(str))\n",
    "                .agg(agg_cols16)\n",
    "                .reset_index()\n",
    "                .rename(columns={\"quadkey\": \"quadkey_z16\"})\n",
    "            )\n",
    "            grid_stats_z16[\"year\"] = year\n",
    "            grid_stats_z16[\"quarter\"] = quarter\n",
    "            grid_stats_z16[\"date\"] = f\"{year}-Q{quarter}\"\n",
    "            grid_stats_z16[\"avg_download_mbps\"] = grid_stats_z16[\"avg_d_kbps\"] / 1000.0\n",
    "            grid_stats_z16[\"avg_upload_mbps\"] = grid_stats_z16[\"avg_u_kbps\"] / 1000.0\n",
    "\n",
    "            keep_z16 = [\"quadkey_z16\", \"year\", \"quarter\", \"date\", \"avg_download_mbps\", \"avg_upload_mbps\"]\n",
    "            if \"avg_lat_ms\" in grid_stats_z16.columns:\n",
    "                keep_z16.append(\"avg_lat_ms\")\n",
    "            if \"tests\" in df_dza.columns:\n",
    "                keep_z16.append(\"tests\")\n",
    "\n",
    "            all_grid_long_z16.append(grid_stats_z16[keep_z16])\n",
    "            print(f\"Grid z16 long rows added: {len(grid_stats_z16)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {year}-Q{quarter}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "# National trends\n",
    "if not all_national:\n",
    "    raise ValueError(\"No national data processed; aborting save.\")\n",
    "df_national = pd.DataFrame(all_national).sort_values([\"year\", \"quarter\"]).reset_index(drop=True)\n",
    "df_national.to_csv(os.path.join(OUTPUT_DIR, \"algeria_national_trends_fixed.csv\"), index=False, encoding=\"utf-8\")\n",
    "print(f\"National trends -> {os.path.join(OUTPUT_DIR, 'algeria_national_trends_fixed.csv')}\")\n",
    "\n",
    "# Subnational trends\n",
    "if all_subnat:\n",
    "    df_sub = pd.concat(all_subnat, ignore_index=True)\n",
    "    df_sub = df_sub.sort_values([\"admin_level\", \"admin_name\", \"year\", \"quarter\"]).reset_index(drop=True)\n",
    "    df_sub.to_csv(os.path.join(OUTPUT_DIR, \"algeria_subnational_trends_fixed.csv\"), index=False, encoding=\"utf-8\")\n",
    "    print(f\"Subnational trends -> {os.path.join(OUTPUT_DIR, 'algeria_subnational_trends_fixed.csv')}\")\n",
    "else:\n",
    "    df_sub = pd.DataFrame()\n",
    "    print(\"No subnational aggregates to save.\")\n",
    "\n",
    "# Grid long (z12)\n",
    "if all_grid_long:\n",
    "    df_grid_long = pd.concat(all_grid_long, ignore_index=True)\n",
    "    df_grid_long.to_csv(os.path.join(OUTPUT_DIR, \"algeria_grid_data_long_z12_fixed.csv\"),\n",
    "                        index=False, encoding=\"utf-8\")\n",
    "    print(f\"Grid z12 long -> {os.path.join(OUTPUT_DIR, 'algeria_grid_data_long_z12_fixed.csv')}\")\n",
    "\n",
    "    print(\"Building z12 time-series WKT CSV\")\n",
    "    dl_wide = df_grid_long.pivot(index=\"quadkey_z12\", columns=\"date\", values=\"avg_download_mbps\")\n",
    "    ul_wide = df_grid_long.pivot(index=\"quadkey_z12\", columns=\"date\", values=\"avg_upload_mbps\")\n",
    "    dl_wide.columns = [f\"download_{c}\" for c in dl_wide.columns]\n",
    "    ul_wide.columns = [f\"upload_{c}\" for c in ul_wide.columns]\n",
    "\n",
    "    parts = [dl_wide, ul_wide]\n",
    "    if \"avg_lat_ms\" in df_grid_long.columns:\n",
    "        lat_wide = df_grid_long.pivot(index=\"quadkey_z12\", columns=\"date\", values=\"avg_lat_ms\")\n",
    "        lat_wide.columns = [f\"latency_{c}\" for c in lat_wide.columns]\n",
    "        parts.append(lat_wide)\n",
    "\n",
    "    grid_wide = pd.concat(parts, axis=1)\n",
    "\n",
    "    if grid_wide.index.name != \"quadkey_z12\":\n",
    "        grid_wide.index.name = \"quadkey_z12\"\n",
    "    grid_wide = grid_wide.reset_index()\n",
    "    grid_wide[\"quadkey_z12\"] = grid_wide[\"quadkey_z12\"].astype(str).str.strip()\n",
    "\n",
    "    grid_wkt = pd.read_csv(os.path.join(OUTPUT_DIR, \"algeria_grid_z12_wkt.csv\"),\n",
    "                           dtype={\"quadkey\": str}).rename(columns={\"quadkey\": \"quadkey_z12\"})\n",
    "    grid_wkt[\"quadkey_z12\"] = grid_wkt[\"quadkey_z12\"].astype(str).str.strip()\n",
    "\n",
    "    grid_merged = grid_wide.merge(grid_wkt[\"quadkey_z12\"].to_frame().join(grid_wkt[\"wkt\"]), on=\"quadkey_z12\", how=\"left\")\n",
    "    out_wkt = os.path.join(OUTPUT_DIR, \"algeria_grid_timeseries_fixed_wkt.csv\")\n",
    "    grid_merged.to_csv(out_wkt, index=False, encoding=\"utf-8\")\n",
    "    print(f\"z12 time-series WKT CSV -> {out_wkt}\")\n",
    "else:\n",
    "    print(\"No grid-long z12 data; skipped z12 wide build.\")\n",
    "\n",
    "if all_grid_long_z16:\n",
    "    df_grid_long_z16 = pd.concat(all_grid_long_z16, ignore_index=True)\n",
    "    df_grid_long_z16.to_csv(os.path.join(OUTPUT_DIR, \"algeria_grid_data_long_z16_fixed.csv\"),\n",
    "                            index=False, encoding=\"utf-8\")\n",
    "    print(f\"Grid z16 long -> {os.path.join(OUTPUT_DIR, 'algeria_grid_data_long_z16_fixed.csv')}\")\n",
    "\n",
    "print(\"Building population-weighted outputs\")\n",
    "\n",
    "GRID_WKT_CSV = os.path.join(OUTPUT_DIR, \"algeria_grid_z12_wkt.csv\")\n",
    "GRID_LONG_Z12 = os.path.join(OUTPUT_DIR, \"algeria_grid_data_long_z12_fixed.csv\")\n",
    "\n",
    "# WorldPop finder\n",
    "tifs = sorted(glob.glob(os.path.join(WORLDPOP_DIR, \"*.tif\")))\n",
    "year_to_tif = {infer_year(fp): fp for fp in tifs if infer_year(fp) in YEARS}\n",
    "\n",
    "# Load tiles (WKT -> geometry)\n",
    "tiles_df = pd.read_csv(GRID_WKT_CSV, dtype={\"quadkey\": str})\n",
    "tiles_df = tiles_df.rename(columns={\"quadkey\": \"quadkey_z12\"})\n",
    "tiles_df[\"geometry\"] = tiles_df[\"wkt\"].apply(wkt.loads)\n",
    "g_tiles_master = gpd.GeoDataFrame(tiles_df[[\"quadkey_z12\", \"geometry\"]].copy(), geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "del tiles_df\n",
    "\n",
    "# Load Ookla z12 long and tidy\n",
    "df_long = pd.read_csv(GRID_LONG_Z12, dtype={\"quadkey_z12\": str})\n",
    "if \"year\" not in df_long.columns and \"date\" in df_long.columns:\n",
    "    df_long[\"year\"] = df_long[\"date\"].astype(str).str[:4].astype(int)\n",
    "df_long = df_long[df_long[\"year\"].between(YEAR_MIN, YEAR_MAX)].copy()\n",
    "df_long[\"avg_download_mbps\"] = df_long[\"avg_download_mbps\"].astype(float)\n",
    "df_long[\"avg_upload_mbps\"] = df_long[\"avg_upload_mbps\"].astype(float)\n",
    "\n",
    "# ADM1 polygons\n",
    "adm1_w84 = boundary_adm1.rename(columns={\"shapeISO\": \"adm1_code\", \"shapeName\": \"adm1_name\"})\n",
    "\n",
    "# Prepare outputs\n",
    "OUT_PW_NAT = os.path.join(OUTPUT_DIR, \"algeria_pop_weighted_trends_2019_2025.csv\")\n",
    "OUT_PW_ADM1 = os.path.join(OUTPUT_DIR, \"algeria_pop_weighted_adm1_z12_2019_2025.csv\")\n",
    "pd.DataFrame(columns=[\"year\", \"pw_download_mbps\", \"pw_upload_mbps\"]).to_csv(OUT_PW_NAT, index=False, encoding=\"utf-8\")\n",
    "pd.DataFrame(columns=[\"year\", \"adm1_code\", \"adm1_name\", \"pw_download_mbps\", \"pw_upload_mbps\"]).to_csv(\n",
    "    OUT_PW_ADM1, index=False, encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "for y in YEARS:\n",
    "    tif = year_to_tif.get(y)\n",
    "    if tif is None:\n",
    "        print(f\"Missing WorldPop {y}; skipping\")\n",
    "        continue\n",
    "\n",
    "    sub = df_long[df_long[\"year\"] == y]\n",
    "    if sub.empty:\n",
    "        print(f\"No Ookla rows {y}; skipping\")\n",
    "        continue\n",
    "\n",
    "    tile_speed = (\n",
    "        sub.groupby(\"quadkey_z12\", as_index=False)\n",
    "        .agg(download=(\"avg_download_mbps\", \"mean\"),\n",
    "             upload=(\"avg_upload_mbps\", \"mean\"))\n",
    "    )\n",
    "    tile_speed[\"quadkey_z12\"] = tile_speed[\"quadkey_z12\"].astype(str).str.strip()\n",
    "\n",
    "    g_tiles = g_tiles_master.merge(tile_speed[[\"quadkey_z12\"]].drop_duplicates(),\n",
    "                                   on=\"quadkey_z12\", how=\"inner\")\n",
    "    if g_tiles.empty:\n",
    "        print(f\"Tiles subset empty {y}; skipping\")\n",
    "        continue\n",
    "\n",
    "    with rasterio.Env(GDAL_CACHEMAX=GDAL_CACHE):\n",
    "        with rasterio.open(tif) as src:\n",
    "            r_crs = src.crs\n",
    "            r_bounds = box(*src.bounds)\n",
    "\n",
    "            tiles_proj = g_tiles.to_crs(r_crs)\n",
    "            adm1_proj = adm1_w84.to_crs(r_crs)\n",
    "\n",
    "            # Clip tiles to raster extent\n",
    "            try:\n",
    "                sidx = tiles_proj.sindex\n",
    "                cand = list(sidx.intersection(r_bounds.bounds))\n",
    "                tiles_proj = tiles_proj.iloc[cand]\n",
    "            except Exception:\n",
    "                pass\n",
    "            tiles_proj = tiles_proj[tiles_proj.intersects(r_bounds)].reset_index(drop=True)\n",
    "            if tiles_proj.empty:\n",
    "                print(f\"No overlapping tiles {y}; skipping\")\n",
    "                continue\n",
    "\n",
    "            cent = tiles_proj.to_crs(3857).copy()\n",
    "            cent[\"geometry\"] = cent.geometry.centroid\n",
    "            cent = cent.to_crs(r_crs)\n",
    "\n",
    "            m1 = gpd.sjoin(\n",
    "                cent[[\"quadkey_z12\", \"geometry\"]],\n",
    "                adm1_proj[[\"adm1_code\", \"adm1_name\", \"geometry\"]],\n",
    "                how=\"left\", predicate=\"within\"\n",
    "            ).drop(columns=\"index_right\").drop(columns=\"geometry\")\n",
    "\n",
    "            unmatched = m1[m1[\"adm1_code\"].isna()][[\"quadkey_z12\"]]\n",
    "            if not unmatched.empty:\n",
    "                cand_pairs = gpd.sjoin(\n",
    "                    tiles_proj.merge(unmatched, on=\"quadkey_z12\", how=\"inner\")[[\"quadkey_z12\", \"geometry\"]],\n",
    "                    adm1_proj[[\"adm1_code\", \"adm1_name\", \"geometry\"]],\n",
    "                    how=\"inner\", predicate=\"intersects\"\n",
    "                ).drop(columns=\"index_right\")\n",
    "                if not cand_pairs.empty:\n",
    "                    adm_geom_map = dict(zip(adm1_proj[\"adm1_code\"], adm1_proj[\"geometry\"]))\n",
    "                    inter_areas = []\n",
    "                    for _, row in cand_pairs.iterrows():\n",
    "                        a = row[\"geometry\"].intersection(adm_geom_map[row[\"adm1_code\"]])\n",
    "                        inter_areas.append(a.area if (a is not None and not a.is_empty) else 0.0)\n",
    "                    cand_pairs[\"area\"] = np.array(inter_areas, dtype=float)\n",
    "                    m2 = (\n",
    "                        cand_pairs.sort_values([\"quadkey_z12\", \"area\"], ascending=[True, False])\n",
    "                        .drop_duplicates(subset=[\"quadkey_z12\"])\n",
    "                    )[[\"quadkey_z12\", \"adm1_code\", \"adm1_name\"]]\n",
    "                else:\n",
    "                    m2 = pd.DataFrame(columns=[\"quadkey_z12\", \"adm1_code\", \"adm1_name\"])\n",
    "            else:\n",
    "                m2 = pd.DataFrame(columns=[\"quadkey_z12\", \"adm1_code\", \"adm1_name\"])\n",
    "\n",
    "            m1_ok = m1.dropna(subset=[\"adm1_code\"])\n",
    "            adm_map = pd.concat([m1_ok, m2], ignore_index=True).drop_duplicates(\"quadkey_z12\")\n",
    "\n",
    "            tiles_adm = tiles_proj.merge(adm_map, on=\"quadkey_z12\", how=\"inner\")[\n",
    "                [\"quadkey_z12\", \"adm1_code\", \"adm1_name\", \"geometry\"]\n",
    "            ]\n",
    "            if tiles_adm.empty:\n",
    "                print(f\"Tile to ADM1 map empty {y}; skipping\")\n",
    "                continue\n",
    "\n",
    "            # zonal_stats in chunks\n",
    "            n = len(tiles_adm)\n",
    "            pop_list = []\n",
    "            for start in range(0, n, CHUNK_SIZE):\n",
    "                end = min(start + CHUNK_SIZE, n)\n",
    "                sub_tiles = tiles_adm.iloc[start:end]\n",
    "                zs = zonal_stats(\n",
    "                    list(sub_tiles[\"geometry\"]),\n",
    "                    tif, stats=[\"sum\"],\n",
    "                    nodata=(src.nodata if src.nodata is not None else None),\n",
    "                    all_touched=True\n",
    "                )\n",
    "                pop = np.fromiter((d[\"sum\"] if (d[\"sum\"] is not None) else 0.0 for d in zs), dtype=np.float64)\n",
    "                pop_list.append(pd.DataFrame({\n",
    "                    \"quadkey_z12\": sub_tiles[\"quadkey_z12\"].values,\n",
    "                    \"adm1_code\": sub_tiles[\"adm1_code\"].values,\n",
    "                    \"adm1_name\": sub_tiles[\"adm1_name\"].values,\n",
    "                    \"pop\": pop\n",
    "                }))\n",
    "            pop_df = pd.concat(pop_list, ignore_index=True)\n",
    "            del pop_list\n",
    "\n",
    "    merged = pop_df.merge(tile_speed, on=\"quadkey_z12\", how=\"left\").fillna({\"pop\": 0.0})\n",
    "    merged[\"dl_w\"] = merged[\"download\"] * merged[\"pop\"]\n",
    "    merged[\"ul_w\"] = merged[\"upload\"] * merged[\"pop\"]\n",
    "\n",
    "    # ADM1 weighted\n",
    "    adm1_year = (\n",
    "        merged.groupby([\"adm1_code\", \"adm1_name\"], as_index=False)\n",
    "        .agg(num_dl=(\"dl_w\", \"sum\"),\n",
    "             num_ul=(\"ul_w\", \"sum\"),\n",
    "             den=(\"pop\", \"sum\"))\n",
    "    )\n",
    "    adm1_year[\"pw_download_mbps\"] = np.where(adm1_year[\"den\"] > 0, adm1_year[\"num_dl\"] / adm1_year[\"den\"], np.nan)\n",
    "    adm1_year[\"pw_upload_mbps\"] = np.where(adm1_year[\"den\"] > 0, adm1_year[\"num_ul\"] / adm1_year[\"den\"], np.nan)\n",
    "    adm1_year[\"year\"] = y\n",
    "    adm1_year[[\"year\", \"adm1_code\", \"adm1_name\", \"pw_download_mbps\", \"pw_upload_mbps\"]] \\\n",
    "        .to_csv(OUT_PW_ADM1, mode=\"a\", header=False, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    # National weighted\n",
    "    den = adm1_year[\"den\"].sum()\n",
    "    nat_pw_dl = adm1_year[\"num_dl\"].sum() / den if den > 0 else np.nan\n",
    "    nat_pw_ul = adm1_year[\"num_ul\"].sum() / den if den > 0 else np.nan\n",
    "    pd.DataFrame([{\"year\": y, \"pw_download_mbps\": nat_pw_dl, \"pw_upload_mbps\": nat_pw_ul}]) \\\n",
    "        .to_csv(OUT_PW_NAT, mode=\"a\", header=False, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    import gc\n",
    "    gc.collect()\n",
    "\n",
    "print(\"PREPROCESSING COMPLETE\")\n",
    "print(f\"Unweighted national: {os.path.join(OUTPUT_DIR, 'algeria_national_trends_fixed.csv')}\")\n",
    "print(f\"Unweighted subnational: {os.path.join(OUTPUT_DIR, 'algeria_subnational_trends_fixed.csv')}\")\n",
    "print(f\"Pop-weighted national: {OUT_PW_NAT}\")\n",
    "print(f\"Pop-weighted ADM1: {OUT_PW_ADM1}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algeria_economic_monitoring",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
